{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87473d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from time import time\n",
    "\n",
    "# --- Scikit-learn ---\n",
    "# Modelos\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Métricas\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "# Utilidades\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV # Para ajuste posterior\n",
    "\n",
    "# --- Modelos de Boosting Avanzados ---\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    print(\"XGBoost no está instalado. Para usarlo: uv pip install xgboost\")\n",
    "    xgb = None\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except ImportError:\n",
    "    print(\"LightGBM no está instalado. Para usarlo: uv pip install lightgbm\")\n",
    "    lgb = None\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "except ImportError:\n",
    "    print(\"CatBoost no está instalado. Para usarlo: uv pip install catboost\")\n",
    "    cb = None\n",
    "\n",
    "# --- Configuración de Rutas ---\n",
    "RUTA_DATOS_PROCESADOS = './datos/output/procesado_para_modelado/'\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dad530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Cargar Datos Procesados ---\n",
    "print(\"--- Cargando Datos Procesados ---\")\n",
    "try:\n",
    "    # Intentar cargar nombres de columnas y luego los CSVs\n",
    "    try:\n",
    "        with open(RUTA_DATOS_PROCESADOS + 'columnas_procesadas_finales.json', 'r') as f:\n",
    "            nombres_columnas_procesadas = json.load(f)\n",
    "        X_train = pd.read_csv(RUTA_DATOS_PROCESADOS + 'X_train_procesado.csv')\n",
    "        X_val = pd.read_csv(RUTA_DATOS_PROCESADOS + 'X_val_procesado.csv')\n",
    "        print(\"Datos X cargados como DataFrames de Pandas.\")\n",
    "    except FileNotFoundError: # Fallback a .npy si CSVs o JSON no están\n",
    "        print(\"Archivos CSV o JSON de columnas no encontrados, intentando cargar .npy...\")\n",
    "        X_train_arr = np.load(RUTA_DATOS_PROCESADOS + 'X_train_procesado.npy', allow_pickle=True)\n",
    "        X_val_arr = np.load(RUTA_DATOS_PROCESADOS + 'X_val_procesado.npy', allow_pickle=True)\n",
    "        # Intentar cargar nombres de columnas para los .npy\n",
    "        try:\n",
    "            with open(RUTA_DATOS_PROCESADOS + 'columnas_procesadas_finales.json', 'r') as f:\n",
    "                nombres_columnas_procesadas = json.load(f)\n",
    "            X_train = pd.DataFrame(X_train_arr, columns=nombres_columnas_procesadas)\n",
    "            X_val = pd.DataFrame(X_val_arr, columns=nombres_columnas_procesadas)\n",
    "            print(\"Datos X cargados de .npy y convertidos a DataFrames de Pandas.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"ADVERTENCIA: Archivo de nombres de columnas no encontrado. X_train y X_val serán arrays NumPy.\")\n",
    "            X_train = X_train_arr\n",
    "            X_val = X_val_arr\n",
    "            \n",
    "    y_train_df = pd.read_csv(RUTA_DATOS_PROCESADOS + 'y_train.csv')\n",
    "    y_val_df = pd.read_csv(RUTA_DATOS_PROCESADOS + 'y_val.csv')\n",
    "    y_train = y_train_df['EXITO']\n",
    "    y_val = y_val_df['EXITO']\n",
    "\n",
    "    print(f\"Forma de X_train: {X_train.shape}, Forma de X_val: {X_val.shape}\")\n",
    "    print(f\"Forma de y_train: {y_train.shape}, Forma de y_val: {y_val.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error fatal al cargar los datos procesados: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a9bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En 09_Modelado_Clasificacion.ipynb, después de cargar X_train, y_train\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"\\nDistribución original de y_train ANTES de SMOTE:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "# X_train puede ser un DataFrame de Pandas o un array NumPy aquí\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nDistribución de y_train_resampled DESPUÉS de SMOTE:\")\n",
    "# Convertir y_train_resampled a Series de Pandas para value_counts si es un array NumPy\n",
    "if isinstance(y_train_resampled, np.ndarray):\n",
    "    y_train_resampled_series = pd.Series(y_train_resampled)\n",
    "    print(y_train_resampled_series.value_counts(normalize=True))\n",
    "else: # Si ya es una Serie de Pandas\n",
    "    print(y_train_resampled.value_counts(normalize=True))\n",
    "\n",
    "print(f\"Shape de X_train_resampled: {X_train_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1882de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Función Auxiliar para Evaluar Modelos ---\n",
    "def evaluar_modelo(nombre_modelo, y_true, y_pred, y_pred_proba=None, print_report=True):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    if print_report:\n",
    "        print(f\"\\n--- Evaluación: {nombre_modelo} ---\")\n",
    "        print(f\"Accuracy: {accuracy:.4f} (Objetivo > 0.80)\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-score: {f1:.4f}\")\n",
    "        if roc_auc is not None:\n",
    "            print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred, zero_division=0))\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix - {nombre_modelo}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "        \n",
    "    return {\"modelo\": nombre_modelo, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"roc_auc\": roc_auc}\n",
    "\n",
    "# Lista para almacenar resultados de todos los modelos\n",
    "resultados_modelos = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44332f66",
   "metadata": {},
   "source": [
    "\n",
    "# SECCIÓN DE MODELOS INDIVIDUALES\n",
    "# Cada modelo se entrena y evalúa en su propia sub-sección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.1 Modelo: Regresión Logística ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== Modelo 3.1: Regresión Logística ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "# --- 3.1.1 Regresión Logística con class_weight='balanced' ---\n",
    "print(\"\\n--- 3.1.1 Regresión Logística con class_weight='balanced' (Datos Originales) ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    log_reg_balanced = LogisticRegression(\n",
    "        random_state=RANDOM_STATE, \n",
    "        solver='liblinear', # Bueno para datasets más pequeños o cuando l1 regularización es deseada\n",
    "        max_iter=1000,      # Aumentar si hay problemas de convergencia\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    log_reg_balanced.fit(X_train, y_train) # Entrenar con X_train original\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (class_weight='balanced'): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_lr_b = log_reg_balanced.predict(X_val)\n",
    "    y_pred_proba_val_lr_b = log_reg_balanced.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    res_lr_b = evaluar_modelo(\n",
    "        \"Regresión Logística (class_weight='balanced')\", \n",
    "        y_val, \n",
    "        y_pred_val_lr_b, \n",
    "        y_pred_proba_val_lr_b\n",
    "    )\n",
    "    resultados_modelos.append(res_lr_b)\n",
    "    # Guardar modelo (opcional)\n",
    "    # joblib.dump(log_reg_balanced, RUTA_DATOS_PROCESADOS + 'modelo_reglog_balanced.joblib')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error con Regresión Logística (class_weight='balanced'): {e}\")\n",
    "\n",
    "# --- 3.1.2 Regresión Logística con Datos Remuestreados (SMOTE) ---\n",
    "print(\"\\n--- 3.1.2 Regresión Logística con Datos Remuestreados (SMOTE) ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    # Cuando se usan datos remuestreados, class_weight='balanced' podría no ser necesario o incluso redundante.\n",
    "    # Se puede probar con y sin él en los datos remuestreados. Aquí lo omitimos.\n",
    "    log_reg_smote = LogisticRegression(\n",
    "        random_state=RANDOM_STATE, \n",
    "        solver='liblinear',\n",
    "        max_iter=1000\n",
    "    )\n",
    "    log_reg_smote.fit(X_train_resampled, y_train_resampled) # Entrenar con datos SMOTE\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (SMOTE): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    # IMPORTANTE: Evaluar siempre en el conjunto de validación ORIGINAL (X_val, y_val)\n",
    "    y_pred_val_lr_smote = log_reg_smote.predict(X_val)\n",
    "    y_pred_proba_val_lr_smote = log_reg_smote.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    res_lr_smote = evaluar_modelo(\n",
    "        \"Regresión Logística (SMOTE)\", \n",
    "        y_val, \n",
    "        y_pred_val_lr_smote, \n",
    "        y_pred_proba_val_lr_smote\n",
    "    )\n",
    "    resultados_modelos.append(res_lr_smote)\n",
    "    # Guardar modelo (opcional)\n",
    "    # joblib.dump(log_reg_smote, RUTA_DATOS_PROCESADOS + 'modelo_reglog_smote.joblib')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error con Regresión Logística (SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Función general para buscar el mejor KNN con ENN\n",
    "# -----------------------------------------------\n",
    "def buscar_mejor_knn_con_enn(nombre_modelo, incluir_smote):\n",
    "    print(f\"\\n--- GridSearchCV: {nombre_modelo} ---\")\n",
    "\n",
    "    try:\n",
    "        steps = []\n",
    "        if incluir_smote:\n",
    "            steps.append(('smote', SMOTE(random_state=42)))\n",
    "        steps.append(('enn', EditedNearestNeighbours(kind_sel='all')))\n",
    "        steps.append(('knn', KNeighborsClassifier(n_jobs=-1)))\n",
    "\n",
    "        pipeline = ImbPipeline(steps=steps)\n",
    "\n",
    "        param_grid = {\n",
    "            'enn__n_neighbors': [1, 3, 5, 7, 9],\n",
    "            'knn__n_neighbors': list(range(1, 21))\n",
    "        }\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=param_grid,\n",
    "            cv=5,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        start_time = time()\n",
    "        grid.fit(X_train, y_train)\n",
    "        end_time = time()\n",
    "\n",
    "        best_params = grid.best_params_\n",
    "        best_k = best_params['knn__n_neighbors']\n",
    "        best_enn_k = best_params['enn__n_neighbors']\n",
    "\n",
    "        print(f\"\\n✔ Mejor configuración para {nombre_modelo}:\")\n",
    "        print(f\" - k para KNN: {best_k}\")\n",
    "        print(f\" - k para ENN: {best_enn_k}\")\n",
    "        print(f\"Tiempo total: {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "        y_pred_val = grid.predict(X_val)\n",
    "        y_proba_val = grid.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        res = evaluar_modelo(\n",
    "            f\"{nombre_modelo} (KNN-k={best_k}, ENN-k={best_enn_k})\",\n",
    "            y_val,\n",
    "            y_pred_val,\n",
    "            y_proba_val\n",
    "        )\n",
    "        resultados_modelos.append(res)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en {nombre_modelo}: {e}\")\n",
    "\n",
    "\n",
    "# ================================\n",
    "# === 1. KNN + ENN (sin SMOTE) ===\n",
    "# ================================\n",
    "buscar_mejor_knn_con_enn(\"KNN + ENN (sin SMOTE)\", incluir_smote=False)\n",
    "\n",
    "# =======================================\n",
    "# === 2. KNN + SMOTE + ENN (completo) ===\n",
    "# =======================================\n",
    "buscar_mejor_knn_con_enn(\"KNN + SMOTE + ENN\", incluir_smote=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0162e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.3 Modelo: Support Vector Machine (SVC) ---\n",
    "# NOTA: SVC puede ser lento, especialmente con kernel no lineal. Empezamos con lineal.\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== Modelo 3.3: Support Vector Machine (SVC) ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "# --- 3.3.1 SVC con class_weight='balanced' (Kernel Lineal) ---\n",
    "print(\"\\n--- 3.3.1 Support Vector Machine (Lineal, class_weight='balanced') ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    svc_linear_balanced = SVC(\n",
    "        kernel='linear', \n",
    "        random_state=RANDOM_STATE, \n",
    "        probability=True, # Necesario para predict_proba\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    svc_linear_balanced.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (Lineal, class_weight='balanced'): {end_time - start_time:.2f} segundos\")\n",
    "    \n",
    "    y_pred_val_svc_lb = svc_linear_balanced.predict(X_val)\n",
    "    y_pred_proba_val_svc_lb = svc_linear_balanced.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    res_svc_lb = evaluar_modelo(\n",
    "        \"SVC (Lineal, class_weight='balanced')\", \n",
    "        y_val, \n",
    "        y_pred_val_svc_lb, \n",
    "        y_pred_proba_val_svc_lb\n",
    "    )\n",
    "    resultados_modelos.append(res_svc_lb)\n",
    "except Exception as e:\n",
    "    print(f\"Error con SVC (Lineal, class_weight='balanced'): {e}\")\n",
    "\n",
    "# --- 3.3.2 SVC con Datos Remuestreados (SMOTE) (Kernel Lineal) ---\n",
    "print(\"\\n--- 3.3.2 Support Vector Machine (Lineal, SMOTE) ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    svc_linear_smote = SVC(\n",
    "        kernel='linear', \n",
    "        random_state=RANDOM_STATE, \n",
    "        probability=True\n",
    "    )\n",
    "    svc_linear_smote.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (Lineal, SMOTE): {end_time - start_time:.2f} segundos\")\n",
    "    \n",
    "    y_pred_val_svc_ls = svc_linear_smote.predict(X_val)\n",
    "    y_pred_proba_val_svc_ls = svc_linear_smote.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    res_svc_ls = evaluar_modelo(\n",
    "        \"SVC (Lineal, SMOTE)\", \n",
    "        y_val, \n",
    "        y_pred_val_svc_ls, \n",
    "        y_pred_proba_val_svc_ls\n",
    "    )\n",
    "    resultados_modelos.append(res_svc_ls)\n",
    "except Exception as e:\n",
    "    print(f\"Error con SVC (Lineal, SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opcional: SVC con kernel RBF - puede ser muy lento)\n",
    "print(\"\\n--- 3.3.3 Support Vector Machine (RBF, SMOTE) ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    svc_rbf_smote = SVC(kernel='rbf', random_state=RANDOM_STATE, probability=True)\n",
    "    svc_rbf_smote.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (RBF, SMOTE): {end_time - start_time:.2f} segundos\")\n",
    "    y_pred_val_svc_rs = svc_rbf_smote.predict(X_val)\n",
    "    y_pred_proba_val_svc_rs = svc_rbf_smote.predict_proba(X_val)[:, 1]\n",
    "    res_svc_rs = evaluar_modelo(\"SVC (RBF, SMOTE)\", y_val, y_pred_val_svc_rs, y_pred_proba_val_svc_rs)\n",
    "    resultados_modelos.append(res_svc_rs)\n",
    "except Exception as e:\n",
    "    print(f\"Error con SVC (RBF, SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.4 Modelo: Árbol de Decisión ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== Modelo 3.4: Árbol de Decisión ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "# --- 3.4.1 Árbol de Decisión con class_weight='balanced' ---\n",
    "print(\"\\n--- 3.4.1 Árbol de Decisión (class_weight='balanced') ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    dt_clf_balanced = DecisionTreeClassifier(random_state=RANDOM_STATE, class_weight='balanced')\n",
    "    dt_clf_balanced.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (class_weight='balanced'): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_dt_b = dt_clf_balanced.predict(X_val)\n",
    "    y_pred_proba_val_dt_b = dt_clf_balanced.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    res_dt_b = evaluar_modelo(\n",
    "        \"Árbol de Decisión (class_weight='balanced')\", \n",
    "        y_val, \n",
    "        y_pred_val_dt_b, \n",
    "        y_pred_proba_val_dt_b\n",
    "    )\n",
    "    resultados_modelos.append(res_dt_b)\n",
    "except Exception as e:\n",
    "    print(f\"Error con Árbol de Decisión (class_weight='balanced'): {e}\")\n",
    "\n",
    "# --- 3.4.2 Árbol de Decisión con Datos Remuestreados (SMOTE) ---\n",
    "print(\"\\n--- 3.4.2 Árbol de Decisión (SMOTE) ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    dt_clf_smote = DecisionTreeClassifier(random_state=RANDOM_STATE) # class_weight no es necesario con SMOTE\n",
    "    dt_clf_smote.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (SMOTE): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_dt_smote = dt_clf_smote.predict(X_val)\n",
    "    y_pred_proba_val_dt_smote = dt_clf_smote.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    res_dt_smote = evaluar_modelo(\n",
    "        \"Árbol de Decisión (SMOTE)\", \n",
    "        y_val, \n",
    "        y_pred_val_dt_smote, \n",
    "        y_pred_proba_val_dt_smote\n",
    "    )\n",
    "    resultados_modelos.append(res_dt_smote)\n",
    "except Exception as e:\n",
    "    print(f\"Error con Árbol de Decisión (SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.5 Modelo: Random Forest ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== Modelo 3.5: Random Forest ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "# --- 3.5.1 Random Forest con class_weight='balanced' ---\n",
    "print(\"\\n--- 3.5.1 Random Forest (class_weight='balanced') ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    rf_clf_balanced = RandomForestClassifier(\n",
    "        n_estimators=100,       # Número de árboles, buen punto de partida\n",
    "        random_state=RANDOM_STATE, \n",
    "        n_jobs=-1,              # Usar todos los procesadores disponibles\n",
    "        class_weight='balanced' # O 'balanced_subsample' si el dataset es muy grande\n",
    "    )\n",
    "    rf_clf_balanced.fit(X_train, y_train) # Entrenar con X_train original\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (class_weight='balanced'): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_rf_b = rf_clf_balanced.predict(X_val)\n",
    "    y_pred_proba_val_rf_b = rf_clf_balanced.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    res_rf_b = evaluar_modelo(\n",
    "        \"Random Forest (class_weight='balanced')\", \n",
    "        y_val, \n",
    "        y_pred_val_rf_b, \n",
    "        y_pred_proba_val_rf_b\n",
    "    )\n",
    "    resultados_modelos.append(res_rf_b)\n",
    "    # joblib.dump(rf_clf_balanced, RUTA_DATOS_PROCESADOS + 'modelo_rf_balanced.joblib')\n",
    "except Exception as e:\n",
    "    print(f\"Error con Random Forest (class_weight='balanced'): {e}\")\n",
    "\n",
    "# --- 3.5.2 Random Forest con Datos Remuestreados (SMOTE) ---\n",
    "print(\"\\n--- 3.5.2 Random Forest (SMOTE) ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    rf_clf_smote = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=RANDOM_STATE, \n",
    "        n_jobs=-1\n",
    "        # class_weight no es usualmente necesario aquí porque SMOTE ya balanceó\n",
    "    )\n",
    "    rf_clf_smote.fit(X_train_resampled, y_train_resampled) # Entrenar con datos SMOTE\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (SMOTE): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_rf_smote = rf_clf_smote.predict(X_val) # Evaluar en X_val original\n",
    "    y_pred_proba_val_rf_smote = rf_clf_smote.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    res_rf_smote = evaluar_modelo(\n",
    "        \"Random Forest (SMOTE)\", \n",
    "        y_val, \n",
    "        y_pred_val_rf_smote, \n",
    "        y_pred_proba_val_rf_smote\n",
    "    )\n",
    "    resultados_modelos.append(res_rf_smote)\n",
    "    # joblib.dump(rf_clf_smote, RUTA_DATOS_PROCESADOS + 'modelo_rf_smote.joblib')\n",
    "except Exception as e:\n",
    "    print(f\"Error con Random Forest (SMOTE): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bfeb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.6 Modelo: Extra Trees Classifier ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== Modelo 3.6: Extra Trees Classifier ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "# --- 3.6.1 Extra Trees con class_weight='balanced' ---\n",
    "print(\"\\n--- 3.6.1 Extra Trees (class_weight='balanced') ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    et_clf_balanced = ExtraTreesClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=RANDOM_STATE, \n",
    "        n_jobs=-1, \n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    et_clf_balanced.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (class_weight='balanced'): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_et_b = et_clf_balanced.predict(X_val)\n",
    "    y_pred_proba_val_et_b = et_clf_balanced.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    res_et_b = evaluar_modelo(\n",
    "        \"Extra Trees (class_weight='balanced')\", \n",
    "        y_val, \n",
    "        y_pred_val_et_b, \n",
    "        y_pred_proba_val_et_b\n",
    "    )\n",
    "    resultados_modelos.append(res_et_b)\n",
    "except Exception as e:\n",
    "    print(f\"Error con Extra Trees (class_weight='balanced'): {e}\")\n",
    "\n",
    "# --- 3.6.2 Extra Trees con Datos Remuestreados (SMOTE) ---\n",
    "print(\"\\n--- 3.6.2 Extra Trees (SMOTE) ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    et_clf_smote = ExtraTreesClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=RANDOM_STATE, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    et_clf_smote.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (SMOTE): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_et_smote = et_clf_smote.predict(X_val)\n",
    "    y_pred_proba_val_et_smote = et_clf_smote.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    res_et_smote = evaluar_modelo(\n",
    "        \"Extra Trees (SMOTE)\", \n",
    "        y_val, \n",
    "        y_pred_val_et_smote, \n",
    "        y_pred_proba_val_et_smote\n",
    "    )\n",
    "    resultados_modelos.append(res_et_smote)\n",
    "except Exception as e:\n",
    "    print(f\"Error con Extra Trees (SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee26b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.7 Modelo: AdaBoost Classifier ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== Modelo 3.7: AdaBoost Classifier ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "# --- 3.7.1 AdaBoost con estimador base con class_weight='balanced' ---\n",
    "print(\"\\n--- 3.7.1 AdaBoost (Estimador Base con class_weight='balanced') ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    # Intentar con un estimador base un poco más complejo\n",
    "    base_estimator_ada_balanced = DecisionTreeClassifier(\n",
    "        max_depth=2, # Probando con max_depth=2 en lugar de 1\n",
    "        class_weight='balanced', \n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    ada_clf_balanced_base = AdaBoostClassifier(\n",
    "        estimator=base_estimator_ada_balanced, # Para sklearn >= 1.2\n",
    "        # base_estimator=base_estimator_ada_balanced, # Para sklearn < 1.2\n",
    "        n_estimators=50,\n",
    "        random_state=RANDOM_STATE,\n",
    "        algorithm='SAMME' # Usar SAMME si SAMME.R da problemas con el base estimator\n",
    "                          # o si el base estimator no tiene un predict_proba robusto\n",
    "    )\n",
    "    ada_clf_balanced_base.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (Estimador Base Bal., max_depth=2): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_ada_bb = ada_clf_balanced_base.predict(X_val)\n",
    "    # AdaBoost con SAMME y DecisionTreeClassifier sí debería tener predict_proba\n",
    "    if hasattr(ada_clf_balanced_base, \"predict_proba\"):\n",
    "        y_pred_proba_val_ada_bb = ada_clf_balanced_base.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba_val_ada_bb = None # No se puede calcular ROC AUC\n",
    "\n",
    "    res_ada_bb = evaluar_modelo(\n",
    "        \"AdaBoost (Base Bal. D2, SAMME)\", \n",
    "        y_val, \n",
    "        y_pred_val_ada_bb, \n",
    "        y_pred_proba_val_ada_bb\n",
    "    )\n",
    "    resultados_modelos.append(res_ada_bb)\n",
    "except Exception as e:\n",
    "    print(f\"Error con AdaBoost (Base Bal.): {e}\")\n",
    "\n",
    "# --- 3.7.2 AdaBoost con Datos Remuestreados (SMOTE) ---\n",
    "print(\"\\n--- 3.7.2 AdaBoost (SMOTE) ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    # Con datos SMOTE, el estimador base no necesita class_weight='balanced'\n",
    "    base_estimator_ada_smote = DecisionTreeClassifier(max_depth=1, random_state=RANDOM_STATE) # Podemos probar con max_depth=1 o 2\n",
    "    \n",
    "    ada_clf_smote = AdaBoostClassifier(\n",
    "        estimator=base_estimator_ada_smote, # o base_estimator para sklearn < 1.2\n",
    "        n_estimators=50, \n",
    "        random_state=RANDOM_STATE,\n",
    "        algorithm='SAMME' # Cambiado a SAMME para asegurar compatibilidad\n",
    "    )\n",
    "    ada_clf_smote.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (SMOTE, SAMME): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_ada_smote = ada_clf_smote.predict(X_val)\n",
    "    if hasattr(ada_clf_smote, \"predict_proba\"):\n",
    "        y_pred_proba_val_ada_smote = ada_clf_smote.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba_val_ada_smote = None\n",
    "\n",
    "    res_ada_smote = evaluar_modelo(\n",
    "        \"AdaBoost (SMOTE, SAMME)\", \n",
    "        y_val, \n",
    "        y_pred_val_ada_smote, \n",
    "        y_pred_proba_val_ada_smote\n",
    "    )\n",
    "    resultados_modelos.append(res_ada_smote)\n",
    "except Exception as e:\n",
    "    print(f\"Error con AdaBoost (SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7460f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.8 Modelo: Gradient Boosting Classifier (Scikit-learn) ---\n",
    "# No tiene class_weight directo. Dependerá de SMOTE.\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== Modelo 3.8: Gradient Boosting Classifier (Scikit-learn) ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "# --- 3.8.1 Gradient Boosting con Datos Originales ---\n",
    "print(\"\\n--- 3.8.1 Gradient Boosting (Datos Originales) ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    gb_clf_orig = GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "    gb_clf_orig.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (Datos Originales): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_gb_orig = gb_clf_orig.predict(X_val)\n",
    "    y_pred_proba_val_gb_orig = gb_clf_orig.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    res_gb_orig = evaluar_modelo(\n",
    "        \"Gradient Boosting (sklearn, Original)\", \n",
    "        y_val, \n",
    "        y_pred_val_gb_orig, \n",
    "        y_pred_proba_val_gb_orig\n",
    "    )\n",
    "    resultados_modelos.append(res_gb_orig)\n",
    "except Exception as e:\n",
    "    print(f\"Error con Gradient Boosting (sklearn, Original): {e}\")\n",
    "\n",
    "# --- 3.8.2 Gradient Boosting con Datos Remuestreados (SMOTE) ---\n",
    "print(\"\\n--- 3.8.2 Gradient Boosting (SMOTE) ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    gb_clf_smote = GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "    gb_clf_smote.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (SMOTE): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_gb_smote = gb_clf_smote.predict(X_val)\n",
    "    y_pred_proba_val_gb_smote = gb_clf_smote.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    res_gb_smote = evaluar_modelo(\n",
    "        \"Gradient Boosting (sklearn, SMOTE)\", \n",
    "        y_val, \n",
    "        y_pred_val_gb_smote, \n",
    "        y_pred_proba_val_gb_smote\n",
    "    )\n",
    "    resultados_modelos.append(res_gb_smote)\n",
    "except Exception as e:\n",
    "    print(f\"Error con Gradient Boosting (sklearn, SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.9 Modelo: Naive Bayes (GaussianNB) ---\n",
    "# No tiene class_weight. Dependerá de SMOTE. Asume features normales.\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== Modelo 3.9: Gaussian Naive Bayes ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "# Necesita arrays NumPy, X_train y X_val ya podrían serlo si la conversión a DF falló antes.\n",
    "X_train_nb_conv = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "X_val_nb_conv = X_val.values if isinstance(X_val, pd.DataFrame) else X_val\n",
    "X_train_resampled_nb_conv = X_train_resampled.values if isinstance(X_train_resampled, pd.DataFrame) else X_train_resampled\n",
    "\n",
    "\n",
    "# --- 3.9.1 Gaussian Naive Bayes con Datos Originales ---\n",
    "print(\"\\n--- 3.9.1 Gaussian Naive Bayes (Datos Originales) ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    gnb_clf_orig = GaussianNB()\n",
    "    gnb_clf_orig.fit(X_train_nb_conv, y_train)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (Datos Originales): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_gnb_orig = gnb_clf_orig.predict(X_val_nb_conv)\n",
    "    y_pred_proba_val_gnb_orig = gnb_clf_orig.predict_proba(X_val_nb_conv)[:, 1]\n",
    "\n",
    "    res_gnb_orig = evaluar_modelo(\n",
    "        \"Gaussian Naive Bayes (Original)\", \n",
    "        y_val, \n",
    "        y_pred_val_gnb_orig, \n",
    "        y_pred_proba_val_gnb_orig\n",
    "    )\n",
    "    resultados_modelos.append(res_gnb_orig)\n",
    "except Exception as e:\n",
    "    print(f\"Error con Gaussian Naive Bayes (Original): {e}\")\n",
    "\n",
    "# --- 3.9.2 Gaussian Naive Bayes con Datos Remuestreados (SMOTE) ---\n",
    "print(\"\\n--- 3.9.2 Gaussian Naive Bayes (SMOTE) ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    gnb_clf_smote = GaussianNB()\n",
    "    gnb_clf_smote.fit(X_train_resampled_nb_conv, y_train_resampled)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (SMOTE): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_gnb_smote = gnb_clf_smote.predict(X_val_nb_conv)\n",
    "    y_pred_proba_val_gnb_smote = gnb_clf_smote.predict_proba(X_val_nb_conv)[:, 1]\n",
    "\n",
    "    res_gnb_smote = evaluar_modelo(\n",
    "        \"Gaussian Naive Bayes (SMOTE)\", \n",
    "        y_val, \n",
    "        y_pred_val_gnb_smote, \n",
    "        y_pred_proba_val_gnb_smote\n",
    "    )\n",
    "    resultados_modelos.append(res_gnb_smote)\n",
    "except Exception as e:\n",
    "    print(f\"Error con Gaussian Naive Bayes (SMOTE): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf60c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.10 Modelo: SGD Classifier (Entrenamiento con Gradiente Descendente Estocástico) ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== Modelo 3.10: SGD Classifier ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "# --- 3.10.1 SGD Classifier con class_weight='balanced' ---\n",
    "print(\"\\n--- 3.10.1 SGD Classifier (Log Loss, class_weight='balanced') ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    sgd_clf_balanced = SGDClassifier(\n",
    "        loss='log_loss', # Para comportamiento similar a Regresión Logística\n",
    "        random_state=RANDOM_STATE, \n",
    "        max_iter=1000, # Iteraciones sobre los datos\n",
    "        tol=1e-3,      # Tolerancia para convergencia\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1      # Paralelizar si es posible\n",
    "    )\n",
    "    sgd_clf_balanced.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (class_weight='balanced'): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_sgd_b = sgd_clf_balanced.predict(X_val)\n",
    "    y_pred_proba_val_sgd_b = sgd_clf_balanced.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    res_sgd_b = evaluar_modelo(\n",
    "        \"SGD Classifier (Log Loss, Bal.)\", \n",
    "        y_val, \n",
    "        y_pred_val_sgd_b, \n",
    "        y_pred_proba_val_sgd_b\n",
    "    )\n",
    "    resultados_modelos.append(res_sgd_b)\n",
    "except Exception as e:\n",
    "    print(f\"Error con SGD Classifier (class_weight='balanced'): {e}\")\n",
    "\n",
    "# --- 3.10.2 SGD Classifier con Datos Remuestreados (SMOTE) ---\n",
    "print(\"\\n--- 3.10.2 SGD Classifier (Log Loss, SMOTE) ---\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    sgd_clf_smote = SGDClassifier(\n",
    "        loss='log_loss', \n",
    "        random_state=RANDOM_STATE, \n",
    "        max_iter=1000, \n",
    "        tol=1e-3,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    sgd_clf_smote.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time()\n",
    "    print(f\"Tiempo de entrenamiento (SMOTE): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    y_pred_val_sgd_smote = sgd_clf_smote.predict(X_val)\n",
    "    y_pred_proba_val_sgd_smote = sgd_clf_smote.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    res_sgd_smote = evaluar_modelo(\n",
    "        \"SGD Classifier (Log Loss, SMOTE)\", \n",
    "        y_val, \n",
    "        y_pred_val_sgd_smote, \n",
    "        y_pred_proba_val_sgd_smote\n",
    "    )\n",
    "    resultados_modelos.append(res_sgd_smote)\n",
    "except Exception as e:\n",
    "    print(f\"Error con SGD Classifier (SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.11 Modelo: XGBoost Classifier ---\n",
    "if xgb:\n",
    "    print(\"\\n\\n==============================================================================\")\n",
    "    print(\"=== Modelo 3.11: XGBoost Classifier ===\")\n",
    "    print(\"==============================================================================\")\n",
    "    \n",
    "    # Calcular scale_pos_weight para datos originales\n",
    "    count_neg_orig, count_pos_orig = y_train.value_counts().sort_index()\n",
    "    scale_pos_w_orig = count_neg_orig / count_pos_orig\n",
    "    print(f\"XGBoost (Original): scale_pos_weight = {scale_pos_w_orig:.2f}\")\n",
    "\n",
    "    # --- 3.11.1 XGBoost con scale_pos_weight (Datos Originales) ---\n",
    "    print(\"\\n--- 3.11.1 XGBoost (scale_pos_weight, Datos Originales) ---\")\n",
    "    try:\n",
    "        start_time = time()\n",
    "        xgb_clf_orig = xgb.XGBClassifier(\n",
    "            random_state=RANDOM_STATE, \n",
    "            use_label_encoder=False, # Configuración recomendada para versiones recientes\n",
    "            eval_metric='logloss',   # Métrica de evaluación interna\n",
    "            scale_pos_weight=scale_pos_w_orig\n",
    "        )\n",
    "        xgb_clf_orig.fit(X_train, y_train)\n",
    "        end_time = time()\n",
    "        print(f\"Tiempo de entrenamiento (scale_pos_weight): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "        y_pred_val_xgb_orig = xgb_clf_orig.predict(X_val)\n",
    "        y_pred_proba_val_xgb_orig = xgb_clf_orig.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        res_xgb_orig = evaluar_modelo(\n",
    "            \"XGBoost (scale_pos_weight)\", \n",
    "            y_val, \n",
    "            y_pred_val_xgb_orig, \n",
    "            y_pred_proba_val_xgb_orig\n",
    "        )\n",
    "        resultados_modelos.append(res_xgb_orig)\n",
    "    except Exception as e:\n",
    "        print(f\"Error con XGBoost (scale_pos_weight): {e}\")\n",
    "\n",
    "    # --- 3.11.2 XGBoost con Datos Remuestreados (SMOTE) ---\n",
    "    print(\"\\n--- 3.11.2 XGBoost (SMOTE) ---\")\n",
    "    try:\n",
    "        start_time = time()\n",
    "        # Con datos SMOTE, scale_pos_weight usualmente no es necesario (o se pone a 1)\n",
    "        xgb_clf_smote = xgb.XGBClassifier(\n",
    "            random_state=RANDOM_STATE, \n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        xgb_clf_smote.fit(X_train_resampled, y_train_resampled)\n",
    "        end_time = time()\n",
    "        print(f\"Tiempo de entrenamiento (SMOTE): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "        y_pred_val_xgb_smote = xgb_clf_smote.predict(X_val)\n",
    "        y_pred_proba_val_xgb_smote = xgb_clf_smote.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        res_xgb_smote = evaluar_modelo(\n",
    "            \"XGBoost (SMOTE)\", \n",
    "            y_val, \n",
    "            y_pred_val_xgb_smote, \n",
    "            y_pred_proba_val_xgb_smote\n",
    "        )\n",
    "        resultados_modelos.append(res_xgb_smote)\n",
    "    except Exception as e:\n",
    "        print(f\"Error con XGBoost (SMOTE): {e}\")\n",
    "else:\n",
    "    print(\"\\n\\nXGBoost no está instalado, se omite esta sección.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.12 Modelo: LightGBM Classifier ---\n",
    "if lgb:\n",
    "    print(\"\\n\\n==============================================================================\")\n",
    "    print(\"=== Modelo 3.12: LightGBM Classifier ===\")\n",
    "    print(\"==============================================================================\")\n",
    "\n",
    "    # --- 3.12.1 LightGBM con class_weight='balanced' (Datos Originales) ---\n",
    "    print(\"\\n--- 3.12.1 LightGBM (class_weight='balanced', Datos Originales) ---\")\n",
    "    try:\n",
    "        start_time = time()\n",
    "        lgbm_clf_balanced = lgb.LGBMClassifier(\n",
    "            random_state=RANDOM_STATE, \n",
    "            n_jobs=-1, \n",
    "            class_weight='balanced' # O is_unbalance=True\n",
    "        )\n",
    "        lgbm_clf_balanced.fit(X_train, y_train)\n",
    "        end_time = time()\n",
    "        print(f\"Tiempo de entrenamiento (class_weight='balanced'): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "        y_pred_val_lgbm_b = lgbm_clf_balanced.predict(X_val)\n",
    "        y_pred_proba_val_lgbm_b = lgbm_clf_balanced.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        res_lgbm_b = evaluar_modelo(\n",
    "            \"LightGBM (class_weight='balanced')\", \n",
    "            y_val, \n",
    "            y_pred_val_lgbm_b, \n",
    "            y_pred_proba_val_lgbm_b\n",
    "        )\n",
    "        resultados_modelos.append(res_lgbm_b)\n",
    "    except Exception as e:\n",
    "        print(f\"Error con LightGBM (class_weight='balanced'): {e}\")\n",
    "\n",
    "    # --- 3.12.2 LightGBM con Datos Remuestreados (SMOTE) ---\n",
    "    print(\"\\n--- 3.12.2 LightGBM (SMOTE) ---\")\n",
    "    try:\n",
    "        start_time = time()\n",
    "        lgbm_clf_smote = lgb.LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        lgbm_clf_smote.fit(X_train_resampled, y_train_resampled)\n",
    "        end_time = time()\n",
    "        print(f\"Tiempo de entrenamiento (SMOTE): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "        y_pred_val_lgbm_smote = lgbm_clf_smote.predict(X_val)\n",
    "        y_pred_proba_val_lgbm_smote = lgbm_clf_smote.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        res_lgbm_smote = evaluar_modelo(\n",
    "            \"LightGBM (SMOTE)\", \n",
    "            y_val, \n",
    "            y_pred_val_lgbm_smote, \n",
    "            y_pred_proba_val_lgbm_smote\n",
    "        )\n",
    "        resultados_modelos.append(res_lgbm_smote)\n",
    "    except Exception as e:\n",
    "        print(f\"Error con LightGBM (SMOTE): {e}\")\n",
    "else:\n",
    "    print(\"\\n\\nLightGBM no está instalado, se omite esta sección.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7512cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.13 Modelo: CatBoost Classifier ---\n",
    "if cb:\n",
    "    print(\"\\n\\n==============================================================================\")\n",
    "    print(\"=== Modelo 3.13: CatBoost Classifier ===\")\n",
    "    print(\"==============================================================================\")\n",
    "    \n",
    "    # Preparar datos para CatBoost (si son DataFrames de Pandas, es mejor)\n",
    "    X_train_cb_df = X_train if isinstance(X_train, pd.DataFrame) else pd.DataFrame(X_train)\n",
    "    X_val_cb_df = X_val if isinstance(X_val, pd.DataFrame) else pd.DataFrame(X_val)\n",
    "    X_train_resampled_cb_df = X_train_resampled if isinstance(X_train_resampled, pd.DataFrame) else pd.DataFrame(X_train_resampled)\n",
    "\n",
    "\n",
    "    # --- 3.13.1 CatBoost con auto_class_weights (Datos Originales) ---\n",
    "    print(\"\\n--- 3.13.1 CatBoost (auto_class_weights, Datos Originales) ---\")\n",
    "    try:\n",
    "        start_time = time()\n",
    "        cb_clf_balanced = cb.CatBoostClassifier(\n",
    "            random_state=RANDOM_STATE, \n",
    "            verbose=0, # Suprime output de entrenamiento\n",
    "            auto_class_weights='Balanced' # O 'SqrtBalanced'\n",
    "        )\n",
    "        cb_clf_balanced.fit(X_train_cb_df, y_train)\n",
    "        end_time = time()\n",
    "        print(f\"Tiempo de entrenamiento (auto_class_weights): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "        y_pred_val_cb_b = cb_clf_balanced.predict(X_val_cb_df)\n",
    "        y_pred_proba_val_cb_b = cb_clf_balanced.predict_proba(X_val_cb_df)[:, 1]\n",
    "\n",
    "        res_cb_b = evaluar_modelo(\n",
    "            \"CatBoost (auto_class_weights)\", \n",
    "            y_val, \n",
    "            y_pred_val_cb_b, \n",
    "            y_pred_proba_val_cb_b\n",
    "        )\n",
    "        resultados_modelos.append(res_cb_b)\n",
    "    except Exception as e:\n",
    "        print(f\"Error con CatBoost (auto_class_weights): {e}\")\n",
    "\n",
    "    # --- 3.13.2 CatBoost con Datos Remuestreados (SMOTE) ---\n",
    "    print(\"\\n--- 3.13.2 CatBoost (SMOTE) ---\")\n",
    "    try:\n",
    "        start_time = time()\n",
    "        cb_clf_smote = cb.CatBoostClassifier(random_state=RANDOM_STATE, verbose=0)\n",
    "        cb_clf_smote.fit(X_train_resampled_cb_df, y_train_resampled)\n",
    "        end_time = time()\n",
    "        print(f\"Tiempo de entrenamiento (SMOTE): {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "        y_pred_val_cb_smote = cb_clf_smote.predict(X_val_cb_df)\n",
    "        y_pred_proba_val_cb_smote = cb_clf_smote.predict_proba(X_val_cb_df)[:, 1]\n",
    "\n",
    "        res_cb_smote = evaluar_modelo(\n",
    "            \"CatBoost (SMOTE)\", \n",
    "            y_val, \n",
    "            y_pred_val_cb_smote, \n",
    "            y_pred_proba_val_cb_smote\n",
    "        )\n",
    "        resultados_modelos.append(res_cb_smote)\n",
    "    except Exception as e:\n",
    "        print(f\"Error con CatBoost (SMOTE): {e}\")\n",
    "else:\n",
    "    print(\"\\n\\nCatBoost no está instalado, se omite esta sección.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758fb920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e6e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Resumen Comparativo de Modelos (al final de todos los modelos) ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== Resumen Comparativo de Modelos (en Conjunto de Validación) ===\")\n",
    "print(\"==============================================================================\")\n",
    "if resultados_modelos:\n",
    "    df_resultados = pd.DataFrame(resultados_modelos)\n",
    "    # Ordenar por F1-score de la clase minoritaria (asumiendo que es clase 0) o ROC AUC\n",
    "    # Para F1 de clase 0, necesitarías extraerlo del classification_report o calcularlo manualmente\n",
    "    # Por ahora, ordenamos por ROC AUC o F1 general.\n",
    "    df_resultados = df_resultados.sort_values(by='roc_auc', ascending=False) \n",
    "    print(df_resultados.to_string())\n",
    "    \n",
    "    df_resultados.to_csv(RUTA_DATOS_PROCESADOS + 'comparacion_modelos_validacion.csv', index=False)\n",
    "    print(f\"\\nTabla de comparación guardada en '{RUTA_DATOS_PROCESADOS}comparacion_modelos_validacion.csv'\")\n",
    "    \n",
    "    plt.figure(figsize=(15, 10)) # Ajustar tamaño para más modelos\n",
    "    sns.barplot(x='accuracy', y='modelo', data=df_resultados, palette='viridis', hue='modelo', dodge=False, legend=False) # Usar hue para colores distintos si es necesario, legend=False para evitar leyenda duplicada\n",
    "    plt.title('Comparación de Accuracy de Modelos (Validación)')\n",
    "    plt.xlabel('Accuracy (Objetivo > 0.80)')\n",
    "    plt.ylabel('Modelo')\n",
    "    plt.xlim(0.0, 1.0) # Accuracy va de 0 a 1\n",
    "    plt.axvline(x=0.80, color='r', linestyle='--', label='Objetivo 80% Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.barplot(x='f1', y='modelo', data=df_resultados, palette='mako', hue='modelo', dodge=False, legend=False)\n",
    "    plt.title('Comparación de F1-score (promedio) de Modelos (Validación)')\n",
    "    plt.xlabel('F1-score')\n",
    "    plt.ylabel('Modelo')\n",
    "    plt.xlim(0.0, 1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"No se generaron resultados de modelos para comparar.\")\n",
    "\n",
    "print(\"\\n--- Modelado y Evaluación Inicial Completados ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, recall_score, f1_score # Para scorers personalizados\n",
    "\n",
    "# Definir un scorer común para optimización, priorizando el balance o la clase minoritaria\n",
    "# Por ejemplo, F1-score macro (promedio de F1 para ambas clases) o ROC AUC\n",
    "# O F1 para la clase 0 (asumiendo 0 es la minoritaria \"No Exitoso\")\n",
    "f1_clase_0_scorer = make_scorer(f1_score, pos_label=0, average='binary', zero_division=0)\n",
    "SCORING_METRIC_OPTIMIZATION = 'roc_auc' # O f1_clase_0_scorer, 'f1_macro', 'recall_weighted', etc.\n",
    "\n",
    "N_ITER_RANDOM_SEARCH = 50 # Número de iteraciones para RandomizedSearchCV (ajustar según tiempo)\n",
    "CV_FOLDS = 5              # Número de folds para validación cruzada (ajustar según tiempo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b552ec3",
   "metadata": {},
   "source": [
    "\n",
    "# INICIO DE AJUSTE DE HIPERPARÁMETROS PARA CADA MODELO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fdbe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.1 AJUSTE: Regresión Logística ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== AJUSTE HIPERPARÁMETROS 3.1: Regresión Logística ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "log_reg_param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear'] # liblinear soporta l1 y l2\n",
    "}\n",
    "\n",
    "# --- 3.1.1 Ajuste RegLog con Datos Originales + class_weight ---\n",
    "print(f\"\\n--- Buscando para RegLog (Original + class_weight), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "log_reg_orig_search = RandomizedSearchCV(\n",
    "    estimator=LogisticRegression(random_state=RANDOM_STATE, max_iter=2000, class_weight='balanced', solver='liblinear'),\n",
    "    param_distributions=log_reg_param_grid,\n",
    "    n_iter=min(N_ITER_RANDOM_SEARCH, 6*2), # Max iteraciones posibles si C y penalty son los únicos que varían\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time()\n",
    "    log_reg_orig_search.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {log_reg_orig_search.best_params_}\")\n",
    "    best_log_reg_orig = log_reg_orig_search.best_estimator_\n",
    "    y_pred_val_lr_b_opt = best_log_reg_orig.predict(X_val)\n",
    "    y_pred_proba_val_lr_b_opt = best_log_reg_orig.predict_proba(X_val)[:, 1]\n",
    "    res_lr_b_opt = evaluar_modelo(f\"RegLog Opt (Orig+CW, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_lr_b_opt, y_pred_proba_val_lr_b_opt)\n",
    "    resultados_modelos.append(res_lr_b_opt)\n",
    "except Exception as e: print(f\"Error en ajuste RegLog (Orig+CW): {e}\")\n",
    "\n",
    "# --- 3.1.2 Ajuste RegLog con Datos SMOTE ---\n",
    "print(f\"\\n--- Buscando para RegLog (SMOTE), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "# Para SMOTE, class_weight no se incluye en el grid ya que los datos están balanceados\n",
    "log_reg_smote_search = RandomizedSearchCV(\n",
    "    estimator=LogisticRegression(random_state=RANDOM_STATE, max_iter=2000, solver='liblinear'), # Sin class_weight\n",
    "    param_distributions=log_reg_param_grid,\n",
    "    n_iter=min(N_ITER_RANDOM_SEARCH, 6*2),\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time()\n",
    "    log_reg_smote_search.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {log_reg_smote_search.best_params_}\")\n",
    "    best_log_reg_smote = log_reg_smote_search.best_estimator_\n",
    "    y_pred_val_lr_smote_opt = best_log_reg_smote.predict(X_val)\n",
    "    y_pred_proba_val_lr_smote_opt = best_log_reg_smote.predict_proba(X_val)[:, 1]\n",
    "    res_lr_smote_opt = evaluar_modelo(f\"RegLog Opt (SMOTE, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_lr_smote_opt, y_pred_proba_val_lr_smote_opt)\n",
    "    resultados_modelos.append(res_lr_smote_opt)\n",
    "except Exception as e: print(f\"Error en ajuste RegLog (SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.2 AJUSTE: K-Nearest Neighbors (KNN) ---\n",
    "# KNN no tiene class_weight. El ajuste se hará sobre datos originales y SMOTE.\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== AJUSTE HIPERPARÁMETROS 3.2: K-Nearest Neighbors (KNN) ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 15, 21],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "# --- 3.2.1 Ajuste KNN con Datos Originales ---\n",
    "print(f\"\\n--- Buscando para KNN (Original), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "knn_orig_search = RandomizedSearchCV(\n",
    "    estimator=KNeighborsClassifier(n_jobs=-1),\n",
    "    param_distributions=knn_param_grid,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time()\n",
    "    knn_orig_search.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {knn_orig_search.best_params_}\")\n",
    "    best_knn_orig = knn_orig_search.best_estimator_\n",
    "    y_pred_val_knn_orig_opt = best_knn_orig.predict(X_val)\n",
    "    y_pred_proba_val_knn_orig_opt = best_knn_orig.predict_proba(X_val)[:, 1]\n",
    "    res_knn_orig_opt = evaluar_modelo(f\"KNN Opt (Original, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_knn_orig_opt, y_pred_proba_val_knn_orig_opt)\n",
    "    resultados_modelos.append(res_knn_orig_opt)\n",
    "except Exception as e: print(f\"Error en ajuste KNN (Original): {e}\")\n",
    "\n",
    "# --- 3.2.2 Ajuste KNN con Datos SMOTE ---\n",
    "print(f\"\\n--- Buscando para KNN (SMOTE), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "knn_smote_search = RandomizedSearchCV(\n",
    "    estimator=KNeighborsClassifier(n_jobs=-1),\n",
    "    param_distributions=knn_param_grid,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE), # CV sobre datos SMOTE\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time()\n",
    "    knn_smote_search.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {knn_smote_search.best_params_}\")\n",
    "    best_knn_smote = knn_smote_search.best_estimator_\n",
    "    y_pred_val_knn_smote_opt = best_knn_smote.predict(X_val)\n",
    "    y_pred_proba_val_knn_smote_opt = best_knn_smote.predict_proba(X_val)[:, 1]\n",
    "    res_knn_smote_opt = evaluar_modelo(f\"KNN Opt (SMOTE, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_knn_smote_opt, y_pred_proba_val_knn_smote_opt)\n",
    "    resultados_modelos.append(res_knn_smote_opt)\n",
    "except Exception as e: print(f\"Error en ajuste KNN (SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.3 AJUSTE: Support Vector Machine (SVC) ---\n",
    "# NOTA: SVC, especialmente con kernel RBF, puede ser MUY lento con datasets que tengan\n",
    "#       muchas features o muchas muestras. El kernel lineal es más rápido.\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== AJUSTE HIPERPARÁMETROS 3.3: Support Vector Machine (SVC) ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "svc_param_grid = {\n",
    "    'C': [0.1, 1, 10], # Parámetro de regularización\n",
    "    'kernel': ['linear', 'rbf'], # Probar ambos kernels\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1], # Parámetro del kernel (relevante para 'rbf')\n",
    "    # class_weight se maneja en el estimador base para la versión con datos originales\n",
    "}\n",
    "# Para RBF, un grid más pequeño podría ser necesario si es muy lento\n",
    "svc_param_grid_rbf_focused = {\n",
    "    'C': [1, 10],\n",
    "    'kernel': ['rbf'],\n",
    "    'gamma': ['scale', 0.01],\n",
    "}\n",
    "\n",
    "\n",
    "# --- 3.3.1 Ajuste SVC (Lineal) con Datos Originales + class_weight ---\n",
    "print(f\"\\n--- Buscando para SVC (Lineal, Original + class_weight), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "svc_linear_orig_param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear']}\n",
    "svc_linear_orig_search = RandomizedSearchCV(\n",
    "    estimator=SVC(random_state=RANDOM_STATE, probability=True, class_weight='balanced'),\n",
    "    param_distributions=svc_linear_orig_param_grid,\n",
    "    n_iter=min(N_ITER_RANDOM_SEARCH, 3*1), # C y kernel\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time()\n",
    "    svc_linear_orig_search.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {svc_linear_orig_search.best_params_}\")\n",
    "    best_svc_linear_orig = svc_linear_orig_search.best_estimator_\n",
    "    y_pred_val_svc_lb_opt = best_svc_linear_orig.predict(X_val)\n",
    "    y_pred_proba_val_svc_lb_opt = best_svc_linear_orig.predict_proba(X_val)[:, 1]\n",
    "    res_svc_lb_opt = evaluar_modelo(f\"SVC Opt (Lin, Orig+CW, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_svc_lb_opt, y_pred_proba_val_svc_lb_opt)\n",
    "    resultados_modelos.append(res_svc_lb_opt)\n",
    "except Exception as e: print(f\"Error en ajuste SVC (Lin, Orig+CW): {e}\")\n",
    "\n",
    "\n",
    "# --- 3.3.2 Ajuste SVC (Lineal) con Datos SMOTE ---\n",
    "print(f\"\\n--- Buscando para SVC (Lineal, SMOTE), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "svc_linear_smote_search = RandomizedSearchCV(\n",
    "    estimator=SVC(random_state=RANDOM_STATE, probability=True), # Sin class_weight\n",
    "    param_distributions=svc_linear_orig_param_grid, # Mismo grid para lineal\n",
    "    n_iter=min(N_ITER_RANDOM_SEARCH, 3*1),\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time()\n",
    "    svc_linear_smote_search.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {svc_linear_smote_search.best_params_}\")\n",
    "    best_svc_linear_smote = svc_linear_smote_search.best_estimator_\n",
    "    y_pred_val_svc_ls_opt = best_svc_linear_smote.predict(X_val)\n",
    "    y_pred_proba_val_svc_ls_opt = best_svc_linear_smote.predict_proba(X_val)[:, 1]\n",
    "    res_svc_ls_opt = evaluar_modelo(f\"SVC Opt (Lin, SMOTE, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_svc_ls_opt, y_pred_proba_val_svc_ls_opt)\n",
    "    resultados_modelos.append(res_svc_ls_opt)\n",
    "except Exception as e: print(f\"Error en ajuste SVC (Lin, SMOTE): {e}\")\n",
    "\n",
    "\n",
    "# --- 3.3.3 Ajuste SVC (RBF) con Datos SMOTE (puede ser lento) ---\n",
    "# Con datos originales y RBF + class_weight podría ser aún más lento y menos efectivo que SMOTE\n",
    "print(f\"\\n--- Buscando para SVC (RBF, SMOTE), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "svc_rbf_smote_search = RandomizedSearchCV(\n",
    "    estimator=SVC(random_state=RANDOM_STATE, probability=True, kernel='rbf'),\n",
    "    param_distributions=svc_param_grid_rbf_focused, # Grid más pequeño para RBF\n",
    "    n_iter=min(N_ITER_RANDOM_SEARCH, 2*2), # C y gamma\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    print(\"ADVERTENCIA: El ajuste de SVC con kernel RBF puede tardar mucho tiempo.\")\n",
    "    start_time = time()\n",
    "    svc_rbf_smote_search.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {svc_rbf_smote_search.best_params_}\")\n",
    "    best_svc_rbf_smote = svc_rbf_smote_search.best_estimator_\n",
    "    y_pred_val_svc_rs_opt = best_svc_rbf_smote.predict(X_val)\n",
    "    y_pred_proba_val_svc_rs_opt = best_svc_rbf_smote.predict_proba(X_val)[:, 1]\n",
    "    res_svc_rs_opt = evaluar_modelo(f\"SVC Opt (RBF, SMOTE, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_svc_rs_opt, y_pred_proba_val_svc_rs_opt)\n",
    "    resultados_modelos.append(res_svc_rs_opt)\n",
    "except Exception as e: print(f\"Error en ajuste SVC (RBF, SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aa74fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.4 AJUSTE: Árbol de Decisión ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== AJUSTE HIPERPARÁMETROS 3.4: Árbol de Decisión ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "dt_param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'class_weight': ['balanced', None] # Para la versión con datos originales\n",
    "}\n",
    "dt_param_grid_smote = dt_param_grid.copy()\n",
    "del dt_param_grid_smote['class_weight']\n",
    "\n",
    "# --- 3.4.1 Ajuste Árbol de Decisión con Datos Originales + class_weight ---\n",
    "print(f\"\\n--- Buscando para Árbol Decisión (Original + class_weight), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "dt_orig_search = RandomizedSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    param_distributions=dt_param_grid,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time()\n",
    "    dt_orig_search.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {dt_orig_search.best_params_}\")\n",
    "    best_dt_orig = dt_orig_search.best_estimator_\n",
    "    y_pred_val_dt_b_opt = best_dt_orig.predict(X_val)\n",
    "    y_pred_proba_val_dt_b_opt = best_dt_orig.predict_proba(X_val)[:, 1]\n",
    "    res_dt_b_opt = evaluar_modelo(f\"Árbol Dec. Opt (Orig+CW, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_dt_b_opt, y_pred_proba_val_dt_b_opt)\n",
    "    resultados_modelos.append(res_dt_b_opt)\n",
    "except Exception as e: print(f\"Error en ajuste Árbol Decisión (Orig+CW): {e}\")\n",
    "\n",
    "# --- 3.4.2 Ajuste Árbol de Decisión con Datos SMOTE ---\n",
    "print(f\"\\n--- Buscando para Árbol Decisión (SMOTE), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "dt_smote_search = RandomizedSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    param_distributions=dt_param_grid_smote, # Sin class_weight\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time()\n",
    "    dt_smote_search.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {dt_smote_search.best_params_}\")\n",
    "    best_dt_smote = dt_smote_search.best_estimator_\n",
    "    y_pred_val_dt_smote_opt = best_dt_smote.predict(X_val)\n",
    "    y_pred_proba_val_dt_smote_opt = best_dt_smote.predict_proba(X_val)[:, 1]\n",
    "    res_dt_smote_opt = evaluar_modelo(f\"Árbol Dec. Opt (SMOTE, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_dt_smote_opt, y_pred_proba_val_dt_smote_opt)\n",
    "    resultados_modelos.append(res_dt_smote_opt)\n",
    "except Exception as e: print(f\"Error en ajuste Árbol Decisión (SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ed55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.5 AJUSTE: Random Forest ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== AJUSTE HIPERPARÁMETROS 3.5: Random Forest ===\") # La numeración seguiría la del script original\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "rf_param_grid_full = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_features': ['sqrt', 'log2', None], # None es igual a todas las features (antiguo 'auto')\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': ['balanced', 'balanced_subsample', None] # Para la versión con datos originales\n",
    "}\n",
    "rf_param_grid_smote_full = rf_param_grid_full.copy()\n",
    "del rf_param_grid_smote_full['class_weight'] # No optimizar class_weight con SMOTE\n",
    "\n",
    "# --- 3.5.1 Ajuste RF con Datos Originales + class_weight ---\n",
    "print(f\"\\n--- Buscando para RF (Original + class_weight), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "rf_orig_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    param_distributions=rf_param_grid_full,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH, # Aumentar si es posible\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time()\n",
    "    rf_orig_search.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {rf_orig_search.best_params_}\")\n",
    "    best_rf_orig_opt = rf_orig_search.best_estimator_\n",
    "    y_pred_val_rf_b_opt = best_rf_orig_opt.predict(X_val)\n",
    "    y_pred_proba_val_rf_b_opt = best_rf_orig_opt.predict_proba(X_val)[:, 1]\n",
    "    res_rf_b_opt = evaluar_modelo(f\"RF Opt (Orig+CW, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_rf_b_opt, y_pred_proba_val_rf_b_opt)\n",
    "    resultados_modelos.append(res_rf_b_opt)\n",
    "except Exception as e: print(f\"Error en ajuste RF (Orig+CW): {e}\")\n",
    "\n",
    "\n",
    "# --- 3.5.2 Ajuste RF con Datos SMOTE ---\n",
    "print(f\"\\n--- Buscando para RF (SMOTE), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "rf_smote_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    param_distributions=rf_param_grid_smote_full, # Sin class_weight\n",
    "    n_iter=N_ITER_RANDOM_SEARCH, # Aumentar si es posible\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time()\n",
    "    rf_smote_search.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {rf_smote_search.best_params_}\")\n",
    "    best_rf_smote_opt = rf_smote_search.best_estimator_\n",
    "    y_pred_val_rf_smote_opt = best_rf_smote_opt.predict(X_val)\n",
    "    y_pred_proba_val_rf_smote_opt = best_rf_smote_opt.predict_proba(X_val)[:, 1]\n",
    "    res_rf_smote_opt = evaluar_modelo(f\"RF Opt (SMOTE, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_rf_smote_opt, y_pred_proba_val_rf_smote_opt)\n",
    "    resultados_modelos.append(res_rf_smote_opt)\n",
    "except Exception as e: print(f\"Error en ajuste RF (SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6df792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.6 AJUSTE: Extra Trees Classifier ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== AJUSTE HIPERPARÁMETROS 3.6: Extra Trees Classifier ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "et_param_grid_full = { # Similar a Random Forest\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'bootstrap': [True, False], # ExtraTrees a menudo se usa con bootstrap=False por defecto\n",
    "    'class_weight': ['balanced', 'balanced_subsample', None]\n",
    "}\n",
    "et_param_grid_smote_full = {k:v for k,v in et_param_grid_full.items() if k != 'class_weight'}\n",
    "if 'bootstrap' in et_param_grid_smote_full : et_param_grid_smote_full['bootstrap'] = [False] # Default de ET\n",
    "\n",
    "# --- 3.6.1 Ajuste Extra Trees con Datos Originales + class_weight ---\n",
    "print(f\"\\n--- Buscando para Extra Trees (Original + class_weight), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "et_orig_search = RandomizedSearchCV(\n",
    "    estimator=ExtraTreesClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    param_distributions=et_param_grid_full,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time(); et_orig_search.fit(X_train, y_train); end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {et_orig_search.best_params_}\")\n",
    "    best_et_orig_opt = et_orig_search.best_estimator_\n",
    "    y_pred_val_et_b_opt = best_et_orig_opt.predict(X_val)\n",
    "    y_pred_proba_val_et_b_opt = best_et_orig_opt.predict_proba(X_val)[:, 1]\n",
    "    res_et_b_opt = evaluar_modelo(f\"ExtraTrees Opt (Orig+CW, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_et_b_opt, y_pred_proba_val_et_b_opt)\n",
    "    resultados_modelos.append(res_et_b_opt)\n",
    "except Exception as e: print(f\"Error en ajuste Extra Trees (Orig+CW): {e}\")\n",
    "\n",
    "# --- 3.6.2 Ajuste Extra Trees con Datos SMOTE ---\n",
    "print(f\"\\n--- Buscando para Extra Trees (SMOTE), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "et_smote_search = RandomizedSearchCV(\n",
    "    estimator=ExtraTreesClassifier(random_state=RANDOM_STATE, n_jobs=-1, bootstrap=False), # bootstrap=False es default\n",
    "    param_distributions=et_param_grid_smote_full,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time(); et_smote_search.fit(X_train_resampled, y_train_resampled); end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {et_smote_search.best_params_}\")\n",
    "    best_et_smote_opt = et_smote_search.best_estimator_\n",
    "    y_pred_val_et_smote_opt = best_et_smote_opt.predict(X_val)\n",
    "    y_pred_proba_val_et_smote_opt = best_et_smote_opt.predict_proba(X_val)[:, 1]\n",
    "    res_et_smote_opt = evaluar_modelo(f\"ExtraTrees Opt (SMOTE, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_et_smote_opt, y_pred_proba_val_et_smote_opt)\n",
    "    resultados_modelos.append(res_et_smote_opt)\n",
    "except Exception as e: print(f\"Error en ajuste Extra Trees (SMOTE): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.7 AJUSTE: AdaBoost Classifier ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== AJUSTE HIPERPARÁMETROS 3.7: AdaBoost Classifier ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "# Parámetros para AdaBoost y su DecisionTreeClassifier base\n",
    "ada_param_grid = {\n",
    "    'estimator__max_depth': [1, 2, 3], # Del DecisionTreeClassifier base\n",
    "    # 'estimator__class_weight': ['balanced', None], # Para la versión con datos originales\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "    'algorithm': ['SAMME'] # SAMME es más robusto si hay problemas con predict_proba del base\n",
    "}\n",
    "# Crear grids separados para la versión original (con class_weight) y SMOTE (sin class_weight en base)\n",
    "ada_param_grid_orig = ada_param_grid.copy()\n",
    "ada_param_grid_orig['estimator__class_weight'] = ['balanced', None]\n",
    "\n",
    "ada_param_grid_smote = ada_param_grid.copy()\n",
    "# No necesitamos 'estimator__class_weight' para SMOTE en el base estimator\n",
    "\n",
    "# --- 3.7.1 Ajuste AdaBoost con Datos Originales (con class_weight en el base) ---\n",
    "print(f\"\\n--- Buscando para AdaBoost (Original, base con class_weight), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "# NOTA: El estimador debe ser un objeto, no la clase, si vas a pasarle parámetros anidados\n",
    "ada_orig_search = RandomizedSearchCV(\n",
    "    estimator=AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE), random_state=RANDOM_STATE),\n",
    "    param_distributions=ada_param_grid_orig,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time(); ada_orig_search.fit(X_train, y_train); end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {ada_orig_search.best_params_}\")\n",
    "    best_ada_orig_opt = ada_orig_search.best_estimator_\n",
    "    y_pred_val_ada_b_opt = best_ada_orig_opt.predict(X_val)\n",
    "    y_pred_proba_val_ada_b_opt = best_ada_orig_opt.predict_proba(X_val)[:, 1] if hasattr(best_ada_orig_opt, \"predict_proba\") else None\n",
    "    res_ada_b_opt = evaluar_modelo(f\"AdaBoost Opt (Orig Base CW, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_ada_b_opt, y_pred_proba_val_ada_b_opt)\n",
    "    resultados_modelos.append(res_ada_b_opt)\n",
    "except Exception as e: print(f\"Error en ajuste AdaBoost (Orig Base CW): {e}\")\n",
    "\n",
    "# --- 3.7.2 Ajuste AdaBoost con Datos SMOTE ---\n",
    "print(f\"\\n--- Buscando para AdaBoost (SMOTE), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "ada_smote_search = RandomizedSearchCV(\n",
    "    estimator=AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE), random_state=RANDOM_STATE), # Base sin class_weight\n",
    "    param_distributions=ada_param_grid_smote,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time(); ada_smote_search.fit(X_train_resampled, y_train_resampled); end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {ada_smote_search.best_params_}\")\n",
    "    best_ada_smote_opt = ada_smote_search.best_estimator_\n",
    "    y_pred_val_ada_smote_opt = best_ada_smote_opt.predict(X_val)\n",
    "    y_pred_proba_val_ada_smote_opt = best_ada_smote_opt.predict_proba(X_val)[:, 1] if hasattr(best_ada_smote_opt, \"predict_proba\") else None\n",
    "    res_ada_smote_opt = evaluar_modelo(f\"AdaBoost Opt (SMOTE, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_ada_smote_opt, y_pred_proba_val_ada_smote_opt)\n",
    "    resultados_modelos.append(res_ada_smote_opt)\n",
    "except Exception as e: print(f\"Error en ajuste AdaBoost (SMOTE): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e0de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.8 AJUSTE: Gradient Boosting Classifier (Scikit-learn) ---\n",
    "# No tiene class_weight. Se prueba con datos originales y SMOTE.\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== AJUSTE HIPERPARÁMETROS 3.8: Gradient Boosting (Scikit-learn) ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "gbc_param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# --- 3.8.1 Ajuste GBC con Datos Originales ---\n",
    "print(f\"\\n--- Buscando para GBC (Original), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "gbc_orig_search = RandomizedSearchCV(\n",
    "    estimator=GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    param_distributions=gbc_param_grid,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time(); gbc_orig_search.fit(X_train, y_train); end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {gbc_orig_search.best_params_}\")\n",
    "    best_gbc_orig_opt = gbc_orig_search.best_estimator_\n",
    "    y_pred_val_gbc_orig_opt = best_gbc_orig_opt.predict(X_val)\n",
    "    y_pred_proba_val_gbc_orig_opt = best_gbc_orig_opt.predict_proba(X_val)[:, 1]\n",
    "    res_gbc_orig_opt = evaluar_modelo(f\"GBC sklearn Opt (Original, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_gbc_orig_opt, y_pred_proba_val_gbc_orig_opt)\n",
    "    resultados_modelos.append(res_gbc_orig_opt)\n",
    "except Exception as e: print(f\"Error en ajuste GBC (Original): {e}\")\n",
    "\n",
    "# --- 3.8.2 Ajuste GBC con Datos SMOTE ---\n",
    "print(f\"\\n--- Buscando para GBC (SMOTE), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "gbc_smote_search = RandomizedSearchCV(\n",
    "    estimator=GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    param_distributions=gbc_param_grid,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time(); gbc_smote_search.fit(X_train_resampled, y_train_resampled); end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {gbc_smote_search.best_params_}\")\n",
    "    best_gbc_smote_opt = gbc_smote_search.best_estimator_\n",
    "    y_pred_val_gbc_smote_opt = best_gbc_smote_opt.predict(X_val)\n",
    "    y_pred_proba_val_gbc_smote_opt = best_gbc_smote_opt.predict_proba(X_val)[:, 1]\n",
    "    res_gbc_smote_opt = evaluar_modelo(f\"GBC sklearn Opt (SMOTE, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_gbc_smote_opt, y_pred_proba_val_gbc_smote_opt)\n",
    "    resultados_modelos.append(res_gbc_smote_opt)\n",
    "except Exception as e: print(f\"Error en ajuste GBC (SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0344b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.9 AJUSTE: Gaussian Naive Bayes (GNB) ---\n",
    "# GNB tiene pocos hiperparámetros, principalmente 'var_smoothing'.\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== AJUSTE HIPERPARÁMETROS 3.9: Gaussian Naive Bayes ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "gnb_param_grid = {'var_smoothing': np.logspace(0,-9, num=100)} # Rango típico para var_smoothing\n",
    "\n",
    "# Convertir a NumPy si no lo son ya, para GNB\n",
    "X_train_nb_conv = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "X_val_nb_conv = X_val.values if isinstance(X_val, pd.DataFrame) else X_val\n",
    "X_train_resampled_nb_conv = X_train_resampled.values if isinstance(X_train_resampled, pd.DataFrame) else X_train_resampled\n",
    "\n",
    "# --- 3.9.1 Ajuste GNB con Datos Originales ---\n",
    "print(f\"\\n--- Buscando para GNB (Original), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "gnb_orig_search = GridSearchCV( # GridSearchCV es factible aquí por el único hiperparámetro\n",
    "    estimator=GaussianNB(),\n",
    "    param_grid=gnb_param_grid,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time(); gnb_orig_search.fit(X_train_nb_conv, y_train); end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {gnb_orig_search.best_params_}\")\n",
    "    best_gnb_orig_opt = gnb_orig_search.best_estimator_\n",
    "    y_pred_val_gnb_orig_opt = best_gnb_orig_opt.predict(X_val_nb_conv)\n",
    "    y_pred_proba_val_gnb_orig_opt = best_gnb_orig_opt.predict_proba(X_val_nb_conv)[:, 1]\n",
    "    res_gnb_orig_opt = evaluar_modelo(f\"GNB Opt (Original, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_gnb_orig_opt, y_pred_proba_val_gnb_orig_opt)\n",
    "    resultados_modelos.append(res_gnb_orig_opt)\n",
    "except Exception as e: print(f\"Error en ajuste GNB (Original): {e}\")\n",
    "\n",
    "# --- 3.9.2 Ajuste GNB con Datos SMOTE ---\n",
    "print(f\"\\n--- Buscando para GNB (SMOTE), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "gnb_smote_search = GridSearchCV(\n",
    "    estimator=GaussianNB(),\n",
    "    param_grid=gnb_param_grid,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time(); gnb_smote_search.fit(X_train_resampled_nb_conv, y_train_resampled); end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {gnb_smote_search.best_params_}\")\n",
    "    best_gnb_smote_opt = gnb_smote_search.best_estimator_\n",
    "    y_pred_val_gnb_smote_opt = best_gnb_smote_opt.predict(X_val_nb_conv)\n",
    "    y_pred_proba_val_gnb_smote_opt = best_gnb_smote_opt.predict_proba(X_val_nb_conv)[:, 1]\n",
    "    res_gnb_smote_opt = evaluar_modelo(f\"GNB Opt (SMOTE, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_gnb_smote_opt, y_pred_proba_val_gnb_smote_opt)\n",
    "    resultados_modelos.append(res_gnb_smote_opt)\n",
    "except Exception as e: print(f\"Error en ajuste GNB (SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d35d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.10 AJUSTE: SGD Classifier ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== AJUSTE HIPERPARÁMETROS 3.10: SGD Classifier ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "sgd_param_grid = {\n",
    "    'loss': ['log_loss', 'hinge', 'modified_huber', 'perceptron'], \n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'alpha': [0.00001, 0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['optimal', 'adaptive'],\n",
    "    'eta0': [0.001, 0.01, 0.1], # Solo si learning_rate es 'invscaling', 'adaptive'\n",
    "    'class_weight': ['balanced', None] # Para versión original\n",
    "}\n",
    "sgd_param_grid_smote = sgd_param_grid.copy()\n",
    "del sgd_param_grid_smote['class_weight']\n",
    "\n",
    "\n",
    "# --- 3.10.1 Ajuste SGD con Datos Originales + class_weight ---\n",
    "print(f\"\\n--- Buscando para SGD (Original + class_weight), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "sgd_orig_search = RandomizedSearchCV(\n",
    "    estimator=SGDClassifier(random_state=RANDOM_STATE, max_iter=2000, tol=1e-4, n_jobs=-1), # Aumentar max_iter y tol\n",
    "    param_distributions=sgd_param_grid,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time(); sgd_orig_search.fit(X_train, y_train); end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {sgd_orig_search.best_params_}\")\n",
    "    best_sgd_orig_opt = sgd_orig_search.best_estimator_\n",
    "    y_pred_val_sgd_b_opt = best_sgd_orig_opt.predict(X_val)\n",
    "    # Solo log_loss y modified_huber tienen predict_proba\n",
    "    if hasattr(best_sgd_orig_opt, \"predict_proba\"):\n",
    "        y_pred_proba_val_sgd_b_opt = best_sgd_orig_opt.predict_proba(X_val)[:, 1]\n",
    "    else: # Para hinge, perceptron, etc., calcular decision_function y escalar si se usa para ROC AUC\n",
    "        try:\n",
    "            y_decision_val_sgd_b_opt = best_sgd_orig_opt.decision_function(X_val)\n",
    "            # Escalar decision function para que esté entre 0 y 1 para ROC AUC (aproximación)\n",
    "            y_pred_proba_val_sgd_b_opt = (y_decision_val_sgd_b_opt - y_decision_val_sgd_b_opt.min()) / (y_decision_val_sgd_b_opt.max() - y_decision_val_sgd_b_opt.min())\n",
    "            if np.isnan(y_pred_proba_val_sgd_b_opt).any(): y_pred_proba_val_sgd_b_opt = None # Si todos los valores son iguales\n",
    "        except:\n",
    "            y_pred_proba_val_sgd_b_opt = None\n",
    "\n",
    "    res_sgd_b_opt = evaluar_modelo(f\"SGD Opt (Orig+CW, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_sgd_b_opt, y_pred_proba_val_sgd_b_opt)\n",
    "    resultados_modelos.append(res_sgd_b_opt)\n",
    "except Exception as e: print(f\"Error en ajuste SGD (Orig+CW): {e}\")\n",
    "\n",
    "\n",
    "# --- 3.10.2 Ajuste SGD con Datos SMOTE ---\n",
    "print(f\"\\n--- Buscando para SGD (SMOTE), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "sgd_smote_search = RandomizedSearchCV(\n",
    "    estimator=SGDClassifier(random_state=RANDOM_STATE, max_iter=2000, tol=1e-4, n_jobs=-1), # Sin class_weight\n",
    "    param_distributions=sgd_param_grid_smote,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time(); sgd_smote_search.fit(X_train_resampled, y_train_resampled); end_time = time()\n",
    "    print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {sgd_smote_search.best_params_}\")\n",
    "    best_sgd_smote_opt = sgd_smote_search.best_estimator_\n",
    "    y_pred_val_sgd_smote_opt = best_sgd_smote_opt.predict(X_val)\n",
    "    if hasattr(best_sgd_smote_opt, \"predict_proba\"):\n",
    "        y_pred_proba_val_sgd_smote_opt = best_sgd_smote_opt.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        try:\n",
    "            y_decision_val_sgd_smote_opt = best_sgd_smote_opt.decision_function(X_val)\n",
    "            y_pred_proba_val_sgd_smote_opt = (y_decision_val_sgd_smote_opt - y_decision_val_sgd_smote_opt.min()) / (y_decision_val_sgd_smote_opt.max() - y_decision_val_sgd_smote_opt.min())\n",
    "            if np.isnan(y_pred_proba_val_sgd_smote_opt).any(): y_pred_proba_val_sgd_smote_opt = None\n",
    "        except:\n",
    "            y_pred_proba_val_sgd_smote_opt = None\n",
    "            \n",
    "    res_sgd_smote_opt = evaluar_modelo(f\"SGD Opt (SMOTE, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_sgd_smote_opt, y_pred_proba_val_sgd_smote_opt)\n",
    "    resultados_modelos.append(res_sgd_smote_opt)\n",
    "except Exception as e: print(f\"Error en ajuste SGD (SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.11 AJUSTE: XGBoost Classifier ---\n",
    "if xgb:\n",
    "    print(\"\\n\\n==============================================================================\")\n",
    "    print(\"=== AJUSTE HIPERPARÁMETROS 3.11: XGBoost Classifier ===\")\n",
    "    print(\"==============================================================================\")\n",
    "\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [100, 200, 300, 400],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5, 6, 7],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0], # Fracción de muestras para entrenar cada árbol\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0], # Fracción de features para entrenar cada árbol\n",
    "        'gamma': [0, 0.1, 0.2, 0.3], # Reducción mínima de pérdida para hacer una partición\n",
    "        'reg_alpha': [0, 0.001, 0.01, 0.1], # L1 regularización\n",
    "        'reg_lambda': [0.1, 1, 10] # L2 regularización\n",
    "        # scale_pos_weight se maneja por separado\n",
    "    }\n",
    "\n",
    "    # --- 3.11.1 Ajuste XGBoost con Datos Originales + scale_pos_weight ---\n",
    "    print(f\"\\n--- Buscando para XGBoost (Original + scale_pos_weight), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "    count_neg_orig, count_pos_orig = y_train.value_counts().sort_index()\n",
    "    scale_pos_w_orig = count_neg_orig / count_pos_orig\n",
    "    print(f\"  Calculado scale_pos_weight para datos originales: {scale_pos_w_orig:.2f}\")\n",
    "    \n",
    "    xgb_orig_search = RandomizedSearchCV(\n",
    "        estimator=xgb.XGBClassifier(random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss', scale_pos_weight=scale_pos_w_orig),\n",
    "        param_distributions=xgb_param_grid, # El grid no incluye scale_pos_weight, ya está en el estimador\n",
    "        n_iter=500, # Aumentar si es posible\n",
    "        cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE),\n",
    "        scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "    try:\n",
    "        start_time = time(); xgb_orig_search.fit(X_train, y_train); end_time = time()\n",
    "        print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {xgb_orig_search.best_params_}\")\n",
    "        best_xgb_orig_opt = xgb_orig_search.best_estimator_\n",
    "        y_pred_val_xgb_b_opt = best_xgb_orig_opt.predict(X_val)\n",
    "        y_pred_proba_val_xgb_b_opt = best_xgb_orig_opt.predict_proba(X_val)[:, 1]\n",
    "        res_xgb_b_opt = evaluar_modelo(f\"XGBoost Opt (Orig+ScalePW, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_xgb_b_opt, y_pred_proba_val_xgb_b_opt)\n",
    "        resultados_modelos.append(res_xgb_b_opt)\n",
    "    except Exception as e: print(f\"Error en ajuste XGBoost (Orig+ScalePW): {e}\")\n",
    "\n",
    "    # --- 3.11.2 Ajuste XGBoost con Datos SMOTE ---\n",
    "    print(f\"\\n--- Buscando para XGBoost (SMOTE), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "    # Para SMOTE, no se necesita scale_pos_weight\n",
    "    xgb_smote_search = RandomizedSearchCV(\n",
    "        estimator=xgb.XGBClassifier(random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss'),\n",
    "        param_distributions=xgb_param_grid, # El mismo grid, scale_pos_weight no se aplica\n",
    "        n_iter=10, # Aumentar si es posible\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE),\n",
    "        scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "    try:\n",
    "        start_time = time(); xgb_smote_search.fit(X_train_resampled, y_train_resampled); end_time = time()\n",
    "        print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {xgb_smote_search.best_params_}\")\n",
    "        best_xgb_smote_opt = xgb_smote_search.best_estimator_\n",
    "        y_pred_val_xgb_smote_opt = best_xgb_smote_opt.predict(X_val)\n",
    "        y_pred_proba_val_xgb_smote_opt = best_xgb_smote_opt.predict_proba(X_val)[:, 1]\n",
    "        res_xgb_smote_opt = evaluar_modelo(f\"XGBoost Opt (SMOTE, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_xgb_smote_opt, y_pred_proba_val_xgb_smote_opt)\n",
    "        resultados_modelos.append(res_xgb_smote_opt)\n",
    "    except Exception as e: print(f\"Error en ajuste XGBoost (SMOTE): {e}\")\n",
    "else:\n",
    "    print(\"\\n\\nXGBoost no está instalado, se omite esta sección de ajuste.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.12 AJUSTE: LightGBM Classifier ---\n",
    "if lgb:\n",
    "    print(\"\\n\\n==============================================================================\")\n",
    "    print(\"=== AJUSTE HIPERPARÁMETROS 3.12: LightGBM Classifier ===\")\n",
    "    print(\"==============================================================================\")\n",
    "\n",
    "    lgbm_param_grid = {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'num_leaves': [20, 31, 40, 50, 60], # <= 2^max_depth\n",
    "        'max_depth': [-1, 10, 15, 20],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],      # Aliases: bagging_fraction\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0], # Aliases: feature_fraction\n",
    "        'reg_alpha': [0, 0.01, 0.1, 0.5], # L1\n",
    "        'reg_lambda': [0, 0.01, 0.1, 0.5], # L2\n",
    "        'class_weight': ['balanced', None] # Para versión original. Alternativa 'is_unbalance': [True, False]\n",
    "    }\n",
    "    lgbm_param_grid_smote = lgbm_param_grid.copy()\n",
    "    del lgbm_param_grid_smote['class_weight']\n",
    "    # lgbm_param_grid_smote['is_unbalance'] = [False] # Si SMOTE ya balanceó\n",
    "\n",
    "    # --- 3.12.1 Ajuste LightGBM con Datos Originales + class_weight ---\n",
    "    print(f\"\\n--- Buscando para LightGBM (Original + class_weight), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "    lgbm_orig_search = RandomizedSearchCV(\n",
    "        estimator=lgb.LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "        param_distributions=lgbm_param_grid,\n",
    "        n_iter=N_ITER_RANDOM_SEARCH,\n",
    "        cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "        scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "    try:\n",
    "        start_time = time(); lgbm_orig_search.fit(X_train, y_train); end_time = time()\n",
    "        print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {lgbm_orig_search.best_params_}\")\n",
    "        best_lgbm_orig_opt = lgbm_orig_search.best_estimator_\n",
    "        y_pred_val_lgbm_b_opt = best_lgbm_orig_opt.predict(X_val)\n",
    "        y_pred_proba_val_lgbm_b_opt = best_lgbm_orig_opt.predict_proba(X_val)[:, 1]\n",
    "        res_lgbm_b_opt = evaluar_modelo(f\"LGBM Opt (Orig+CW, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_lgbm_b_opt, y_pred_proba_val_lgbm_b_opt)\n",
    "        resultados_modelos.append(res_lgbm_b_opt)\n",
    "    except Exception as e: print(f\"Error en ajuste LightGBM (Orig+CW): {e}\")\n",
    "\n",
    "    # --- 3.12.2 Ajuste LightGBM con Datos SMOTE ---\n",
    "    print(f\"\\n--- Buscando para LightGBM (SMOTE), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "    lgbm_smote_search = RandomizedSearchCV(\n",
    "        estimator=lgb.LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1), # Sin class_weight\n",
    "        param_distributions=lgbm_param_grid_smote,\n",
    "        n_iter=N_ITER_RANDOM_SEARCH,\n",
    "        cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "        scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "    try:\n",
    "        start_time = time(); lgbm_smote_search.fit(X_train_resampled, y_train_resampled); end_time = time()\n",
    "        print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {lgbm_smote_search.best_params_}\")\n",
    "        best_lgbm_smote_opt = lgbm_smote_search.best_estimator_\n",
    "        y_pred_val_lgbm_smote_opt = best_lgbm_smote_opt.predict(X_val)\n",
    "        y_pred_proba_val_lgbm_smote_opt = best_lgbm_smote_opt.predict_proba(X_val)[:, 1]\n",
    "        res_lgbm_smote_opt = evaluar_modelo(f\"LGBM Opt (SMOTE, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_lgbm_smote_opt, y_pred_proba_val_lgbm_smote_opt)\n",
    "        resultados_modelos.append(res_lgbm_smote_opt)\n",
    "    except Exception as e: print(f\"Error en ajuste LightGBM (SMOTE): {e}\")\n",
    "else:\n",
    "    print(\"\\n\\nLightGBM no está instalado, se omite esta sección de ajuste.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.13 AJUSTE: CatBoost Classifier ---\n",
    "if cb:\n",
    "    print(\"\\n\\n==============================================================================\")\n",
    "    print(\"=== AJUSTE HIPERPARÁMETROS 3.13: CatBoost Classifier ===\")\n",
    "    print(\"==============================================================================\")\n",
    "\n",
    "    cb_param_grid = {\n",
    "        'iterations': [100, 200, 300, 500],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "        'depth': [4, 6, 8, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5, 9], # Regularización L2\n",
    "        'border_count': [32, 64, 128], # Número de divisiones para features numéricas\n",
    "        'auto_class_weights': ['Balanced', 'SqrtBalanced', None] # Para versión original\n",
    "    }\n",
    "    cb_param_grid_smote = cb_param_grid.copy()\n",
    "    del cb_param_grid_smote['auto_class_weights']\n",
    "    \n",
    "    # CatBoost prefiere DataFrames de Pandas si se usan feature_names\n",
    "    X_train_cb_df = X_train if isinstance(X_train, pd.DataFrame) else pd.DataFrame(X_train, columns=nombres_columnas_procesadas if 'nombres_columnas_procesadas' in locals() else None)\n",
    "    X_val_cb_df = X_val if isinstance(X_val, pd.DataFrame) else pd.DataFrame(X_val, columns=nombres_columnas_procesadas if 'nombres_columnas_procesadas' in locals() else None)\n",
    "    X_train_resampled_cb_df = X_train_resampled if isinstance(X_train_resampled, pd.DataFrame) else pd.DataFrame(X_train_resampled, columns=nombres_columnas_procesadas if 'nombres_columnas_procesadas' in locals() else None)\n",
    "\n",
    "\n",
    "    # --- 3.13.1 Ajuste CatBoost con Datos Originales + auto_class_weights ---\n",
    "    print(f\"\\n--- Buscando para CatBoost (Original + auto_class_weights), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "    cb_orig_search = RandomizedSearchCV(\n",
    "        estimator=cb.CatBoostClassifier(random_state=RANDOM_STATE, verbose=0, thread_count=-1),\n",
    "        param_distributions=cb_param_grid,\n",
    "        n_iter=5,\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE),\n",
    "        scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE # n_jobs no es para CatBoostSearchCV\n",
    "    )\n",
    "    try:\n",
    "        start_time = time(); cb_orig_search.fit(X_train_cb_df, y_train); end_time = time() # Pasando feature_names si X_train es DF\n",
    "        print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {cb_orig_search.best_params_}\")\n",
    "        best_cb_orig_opt = cb_orig_search.best_estimator_\n",
    "        y_pred_val_cb_b_opt = best_cb_orig_opt.predict(X_val_cb_df)\n",
    "        y_pred_proba_val_cb_b_opt = best_cb_orig_opt.predict_proba(X_val_cb_df)[:, 1]\n",
    "        res_cb_b_opt = evaluar_modelo(f\"CatBoost Opt (Orig+ACW, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_cb_b_opt, y_pred_proba_val_cb_b_opt)\n",
    "        resultados_modelos.append(res_cb_b_opt)\n",
    "    except Exception as e: print(f\"Error en ajuste CatBoost (Orig+ACW): {e}\")\n",
    "\n",
    "    # --- 3.13.2 Ajuste CatBoost con Datos SMOTE ---\n",
    "    print(f\"\\n--- Buscando para CatBoost (SMOTE), optimizando para '{SCORING_METRIC_OPTIMIZATION}' ---\")\n",
    "    cb_smote_search = RandomizedSearchCV(\n",
    "        estimator=cb.CatBoostClassifier(random_state=RANDOM_STATE, verbose=0, thread_count=-1), # Sin auto_class_weights\n",
    "        param_distributions=cb_param_grid_smote,\n",
    "        n_iter=5,\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE),\n",
    "        scoring=SCORING_METRIC_OPTIMIZATION, verbose=1, random_state=RANDOM_STATE\n",
    "    )\n",
    "    try:\n",
    "        start_time = time(); cb_smote_search.fit(X_train_resampled_cb_df, y_train_resampled); end_time = time()\n",
    "        print(f\"Búsqueda completada en {end_time - start_time:.2f}s. Mejores params: {cb_smote_search.best_params_}\")\n",
    "        best_cb_smote_opt = cb_smote_search.best_estimator_\n",
    "        y_pred_val_cb_smote_opt = best_cb_smote_opt.predict(X_val_cb_df)\n",
    "        y_pred_proba_val_cb_smote_opt = best_cb_smote_opt.predict_proba(X_val_cb_df)[:, 1]\n",
    "        res_cb_smote_opt = evaluar_modelo(f\"CatBoost Opt (SMOTE, Scored by {SCORING_METRIC_OPTIMIZATION})\", y_val, y_pred_val_cb_smote_opt, y_pred_proba_val_cb_smote_opt)\n",
    "        resultados_modelos.append(res_cb_smote_opt)\n",
    "    except Exception as e: print(f\"Error en ajuste CatBoost (SMOTE): {e}\")\n",
    "else:\n",
    "    print(\"\\n\\nCatBoost no está instalado, se omite esta sección de ajuste.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c52ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Resumen Comparativo FINAL de Modelos (Incluyendo Optimizados) ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== Resumen Comparativo FINAL de Modelos (Incluyendo Optimizados) ===\")\n",
    "print(\"==============================================================================\")\n",
    "if resultados_modelos:\n",
    "    df_resultados_final = pd.DataFrame(resultados_modelos)\n",
    "    df_resultados_final = df_resultados_final.sort_values(by=SCORING_METRIC_OPTIMIZATION, ascending=False) \n",
    "    print(df_resultados_final.to_string())\n",
    "    \n",
    "    # Guardar resultados a CSV\n",
    "    df_resultados_final.to_csv(RUTA_DATOS_PROCESADOS + 'comparacion_modelos_final_optimizados.csv', index=False)\n",
    "    print(f\"\\nTabla de comparación final guardada en '{RUTA_DATOS_PROCESADOS}comparacion_modelos_final_optimizados.csv'\")\n",
    "    \n",
    "    # Gráfico de barras para Accuracy\n",
    "    plt.figure(figsize=(15, max(10, len(df_resultados_final) * 0.5))) # Ajustar altura dinámicamente\n",
    "    sns.barplot(x='accuracy', y='modelo', data=df_resultados_final, palette='viridis', hue='modelo', dodge=False, legend=False)\n",
    "    plt.title('Comparación de Accuracy de Modelos Optimizados (Validación)')\n",
    "    plt.xlabel('Accuracy (Objetivo > 0.80)')\n",
    "    plt.ylabel('Modelo')\n",
    "    plt.xlim(min(0.5, df_resultados_final['accuracy'].min() - 0.05) , 1.0) \n",
    "    plt.axvline(x=0.80, color='r', linestyle='--', label='Objetivo 80% Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Gráfico de barras para ROC AUC (o la métrica de optimización)\n",
    "    if SCORING_METRIC_OPTIMIZATION in df_resultados_final.columns and df_resultados_final[SCORING_METRIC_OPTIMIZATION].notnull().any():\n",
    "        plt.figure(figsize=(15, max(10, len(df_resultados_final) * 0.5)))\n",
    "        sns.barplot(x=SCORING_METRIC_OPTIMIZATION, y='modelo', data=df_resultados_final, palette='mako', hue='modelo', dodge=False, legend=False)\n",
    "        plt.title(f'Comparación de {SCORING_METRIC_OPTIMIZATION.upper()} de Modelos Optimizados (Validación)')\n",
    "        plt.xlabel(SCORING_METRIC_OPTIMIZATION.upper())\n",
    "        plt.ylabel('Modelo')\n",
    "        plt.xlim(min(0.0, df_resultados_final[SCORING_METRIC_OPTIMIZATION].min() - 0.05) , 1.0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No se generaron resultados de modelos para comparar.\")\n",
    "\n",
    "print(\"\\n--- Ajuste de Hiperparámetros y Evaluación Completados ---\")\n",
    "print(\"Revisa 'comparacion_modelos_final_optimizados.csv' para seleccionar el mejor modelo.\")\n",
    "print(\"El siguiente paso sería entrenar el MEJOR modelo con TODOS los datos de entrenamiento disponibles (X_train + X_val),\")\n",
    "print(\"y luego usarlo para predecir en el X_test_oficial_procesado y en el conjunto de evaluación del jurado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce37b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, recall_score, f1_score, balanced_accuracy_score\n",
    "import xgboost as xgb\n",
    "from time import time\n",
    "\n",
    "# Definir scorers personalizados enfocados en la clase minoritaria (0)\n",
    "recall_clase_0_scorer = make_scorer(recall_score, pos_label=0, zero_division=0)\n",
    "f1_clase_0_scorer = make_scorer(f1_score, pos_label=0, zero_division=0)\n",
    "# balanced_accuracy_scorer ya existe en sklearn.metrics\n",
    "\n",
    "# --- RE-AJUSTE: XGBoost Classifier (Datos Originales + scale_pos_weight) ---\n",
    "# --- Enfocado en mejorar la predicción de la clase minoritaria (EXITO=0) ---\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== RE-AJUSTE HIPERPARÁMETROS: XGBoost (Orig+ScalePW) - Foco Clase Minoritaria ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "# Calcular scale_pos_weight para datos originales (y_train)\n",
    "count_neg_orig, count_pos_orig = y_train.value_counts().sort_index()\n",
    "# Si la clase 0 es la minoritaria, scale_pos_weight es count_majority / count_minority\n",
    "# En tu caso, EXITO=1 es mayoritario, EXITO=0 es minoritario.\n",
    "# scale_pos_weight se aplica a la clase positiva (1). Para darle más peso a la negativa (0),\n",
    "# algunos lo interpretan como: si la clase positiva es la minoritaria, scale_pos_weight > 1.\n",
    "# Si la clase positiva es la mayoritaria (como en tu caso), scale_pos_weight < 1 para \"reducir\" su peso\n",
    "# o, más comúnmente, se usa para *aumentar* el peso de la clase positiva si esta es la minoritaria.\n",
    "# Dado que EXITO=1 es la mayoritaria, scale_pos_weight = count_neg_orig / count_pos_orig le dará MENOS peso a la clase 1.\n",
    "# Esto es correcto si queremos que el modelo preste más atención a la clase 0.\n",
    "scale_pos_w_orig = count_neg_orig / count_pos_orig \n",
    "print(f\"XGBoost (Original): Usando scale_pos_weight = {scale_pos_w_orig:.4f}\")\n",
    "\n",
    "# Espacio de búsqueda de hiperparámetros para XGBoost\n",
    "# Podríamos necesitar tasas de aprendizaje más bajas, más árboles, o diferente max_depth\n",
    "# para capturar mejor la clase minoritaria. También podemos probar diferentes valores de scale_pos_weight.\n",
    "xgb_param_grid_refined = {\n",
    "    'n_estimators': [200, 300, 400, 500],\n",
    "    'learning_rate': [0.01, 0.02, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8], # Probar profundidades variadas\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.5],\n",
    "    'reg_alpha': [0, 0.001, 0.01, 0.1],\n",
    "    'reg_lambda': [0.1, 0.5, 1, 5, 10],\n",
    "    # scale_pos_weight se define en el estimador base, pero podríamos incluirlo en el grid\n",
    "    # para que RandomizedSearchCV también lo explore si queremos.\n",
    "    # 'scale_pos_weight': [scale_pos_w_orig, scale_pos_w_orig * 0.5, scale_pos_w_orig * 2] # Ejemplo de exploración\n",
    "}\n",
    "\n",
    "# Métrica de optimización: F1-score para la clase 0\n",
    "SCORING_METRIC_REFINED = f1_clase_0_scorer \n",
    "# Otras opciones: SCORING_METRIC_REFINED = recall_clase_0_scorer\n",
    "#              SCORING_METRIC_REFINED = 'f1_macro'\n",
    "#              SCORING_METRIC_REFINED = 'balanced_accuracy'\n",
    "\n",
    "print(f\"\\n--- Re-buscando para XGBoost (Original + scale_pos_weight), optimizando para F1-score de la clase 0 ---\")\n",
    "\n",
    "xgb_orig_refined_search = RandomizedSearchCV(\n",
    "    estimator=xgb.XGBClassifier(\n",
    "        random_state=RANDOM_STATE, \n",
    "        use_label_encoder=False, # obsoleto, mejor no usar o poner False explícitamente\n",
    "        eval_metric='logloss', # o 'aucpr' para desbalance\n",
    "        scale_pos_weight=scale_pos_w_orig # Usar el calculado\n",
    "    ),\n",
    "    param_distributions=xgb_param_grid_refined,\n",
    "    n_iter=30, # Aumentar n_iter para una búsqueda más exhaustiva si el tiempo lo permite\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE), # CV_FOLDS = 3 o 5\n",
    "    scoring=SCORING_METRIC_REFINED, \n",
    "    verbose=1, \n",
    "    random_state=RANDOM_STATE, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "try:\n",
    "    start_time = time()\n",
    "    xgb_orig_refined_search.fit(X_train, y_train) # Entrenar con X_train original\n",
    "    end_time = time()\n",
    "    print(f\"Búsqueda refinada completada en {end_time - start_time:.2f}s.\")\n",
    "    print(f\"Mejores parámetros para XGBoost (Orig+ScalePW, Foco Clase 0): {xgb_orig_refined_search.best_params_}\")\n",
    "    print(f\"Mejor puntuación '{str(SCORING_METRIC_REFINED)}' (CV): {xgb_orig_refined_search.best_score_:.4f}\")\n",
    "\n",
    "    best_xgb_orig_refined = xgb_orig_refined_search.best_estimator_\n",
    "    \n",
    "    # Evaluar este nuevo mejor modelo en el conjunto de validación\n",
    "    y_pred_val_xgb_refined = best_xgb_orig_refined.predict(X_val)\n",
    "    y_pred_proba_val_xgb_refined = best_xgb_orig_refined.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    res_xgb_refined = evaluar_modelo(\n",
    "        f\"XGBoost REFINADO (Orig+ScalePW, Scored by {str(SCORING_METRIC_REFINED)})\", \n",
    "        y_val, \n",
    "        y_pred_val_xgb_refined, \n",
    "        y_pred_proba_val_xgb_refined\n",
    "    )\n",
    "    # Podrías añadir este resultado a una nueva lista o compararlo directamente\n",
    "    print(\"\\nResultados del Modelo XGBoost Refinado en Validación:\")\n",
    "    for key, value in res_xgb_refined.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "            \n",
    "    # Guardar este modelo refinado\n",
    "    joblib.dump(best_xgb_orig_refined, RUTA_DATOS_PROCESADOS + 'modelo_xgb_refinado_clase0.joblib')\n",
    "    print(f\"Modelo XGBoost refinado guardado en '{RUTA_DATOS_PROCESADOS}modelo_xgb_refinado_clase0.joblib'\")\n",
    "\n",
    "except Exception as e: \n",
    "    print(f\"Error en el re-ajuste de XGBoost (Orig+ScalePW): {e}\")\n",
    "    raise # Levantar el error para ver el traceback completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e4b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OPCIONAL: Probar XGBoost con SMOTE y optimizar para F1-score de clase 0 ---\n",
    "# Esto es por si la estrategia con scale_pos_weight no da los resultados esperados\n",
    "# para la clase minoritaria incluso después de optimizar para su F1.\n",
    "\n",
    "xgb_param_grid_smote_refined = xgb_param_grid_refined.copy()\n",
    "if 'scale_pos_weight' in xgb_param_grid_smote_refined: # No lo necesitamos con SMOTE\n",
    "    del xgb_param_grid_smote_refined['scale_pos_weight']\n",
    "\n",
    "print(f\"\\n--- Re-buscando para XGBoost (SMOTE), optimizando para F1-score de la clase 0 ---\")\n",
    "xgb_smote_refined_search = RandomizedSearchCV(\n",
    "    estimator=xgb.XGBClassifier(random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss'),\n",
    "    param_distributions=xgb_param_grid_smote_refined,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH, \n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=SCORING_METRIC_REFINED, \n",
    "    verbose=1, \n",
    "    random_state=RANDOM_STATE, \n",
    "    n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    start_time = time()\n",
    "    xgb_smote_refined_search.fit(X_train_resampled, y_train_resampled) # Usar datos SMOTE\n",
    "    end_time = time()\n",
    "    print(f\"Búsqueda SMOTE completada en {end_time - start_time:.2f}s.\")\n",
    "    print(f\"Mejores parámetros para XGBoost (SMOTE, Foco Clase 0): {xgb_smote_refined_search.best_params_}\")\n",
    "    print(f\"Mejor puntuación '{str(SCORING_METRIC_REFINED)}' (CV): {xgb_smote_refined_search.best_score_:.4f}\")\n",
    "\n",
    "    best_xgb_smote_refined = xgb_smote_refined_search.best_estimator_\n",
    "    \n",
    "    y_pred_val_xgb_smote_refined = best_xgb_smote_refined.predict(X_val)\n",
    "    y_pred_proba_val_xgb_smote_refined = best_xgb_smote_refined.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    res_xgb_smote_refined = evaluar_modelo(\n",
    "        f\"XGBoost REFINADO (SMOTE, Scored by {str(SCORING_METRIC_REFINED)})\", \n",
    "        y_val, \n",
    "        y_pred_val_xgb_smote_refined, \n",
    "        y_pred_proba_val_xgb_smote_refined\n",
    "    )\n",
    "    # Añadir a resultados_modelos si quieres compararlo con los anteriores\n",
    "    resultados_modelos.append(res_xgb_smote_refined) \n",
    "    print(\"\\nResultados del Modelo XGBoost Refinado con SMOTE en Validación:\")\n",
    "    for key, value in res_xgb_smote_refined.items():\n",
    "        if isinstance(value, float): print(f\"  {key}: {value:.4f}\")\n",
    "        else: print(f\"  {key}: {value}\")\n",
    "except Exception as e: \n",
    "    print(f\"Error en el re-ajuste de XGBoost (SMOTE): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b170d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== ENTRENAMIENTO DEL MODELO FINAL (XGBoost con Mejores Hiperparámetros) ===\")\n",
    "print(\"==============================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar X_train_full_raw y y_train_full (los datos ANTES del split train/val)\n",
    "# Asumimos que X_train_full_raw está preprocesado (escalado, OHE) usando el preprocesador\n",
    "# ajustado con el X_train_raw_split original.\n",
    "# Si no guardaste X_train_full_raw procesado, recárgalo y aplica el preprocesador.joblib.\n",
    "\n",
    "# Si X_train (el del split) es representativo, podrías re-entrenar con él.\n",
    "# Pero para \"producción\", usar todos los datos de entrenamiento disponibles es común.\n",
    "# Para este ejemplo, re-entrenaremos con X_train y y_train (el split más grande).\n",
    "# Si quieres usar TODO el dato de entrenamiento original (X_train_full_raw),\n",
    "# necesitarías aplicar el preprocesador.fit_transform() a X_train_full_raw.\n",
    "# Por simplicidad aquí, y para ser consistente con el RandomizedSearchCV,\n",
    "# usaremos el X_train que ya está procesado y con el que se hizo la búsqueda.\n",
    "\n",
    "best_xgb_params = {\n",
    "    'subsample': 0.9, \n",
    "    'reg_lambda': 0.1, \n",
    "    'reg_alpha': 0.001, \n",
    "    'n_estimators': 300, \n",
    "    'max_depth': 7, \n",
    "    'learning_rate': 0.01, \n",
    "    'gamma': 0.2, \n",
    "    'colsample_bytree': 1.0\n",
    "}\n",
    "\n",
    "# Recalcular scale_pos_weight usando y_train (el conjunto de entrenamiento del split)\n",
    "count_neg_final_train, count_pos_final_train = y_train.value_counts().sort_index()\n",
    "scale_pos_w_final_train = count_neg_final_train / count_pos_final_train\n",
    "print(f\"Usando scale_pos_weight para el modelo final: {scale_pos_w_final_train:.2f}\")\n",
    "\n",
    "modelo_final_xgb = xgb.XGBClassifier(\n",
    "    **best_xgb_params,\n",
    "    scale_pos_weight=scale_pos_w_final_train,\n",
    "    random_state=RANDOM_STATE,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando modelo final XGBoost con X_train y y_train...\")\n",
    "start_time = time()\n",
    "modelo_final_xgb.fit(X_train, y_train) # X_train y y_train son los del split, ya procesados\n",
    "end_time = time()\n",
    "print(f\"Modelo final entrenado en {end_time - start_time:.2f} segundos.\")\n",
    "\n",
    "# Guardar el modelo final entrenado\n",
    "joblib.dump(modelo_final_xgb, RUTA_DATOS_PROCESADOS + 'modelo_final_xgb.joblib')\n",
    "print(f\"Modelo final XGBoost guardado en '{RUTA_DATOS_PROCESADOS}modelo_final_xgb.joblib'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Importancia de Características del Modelo Final XGBoost ---\")\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    feature_importances = pd.Series(modelo_final_xgb.feature_importances_, index=X_train.columns)\n",
    "    importances_plot = feature_importances.nlargest(30).sort_values().plot(kind='barh', figsize=(10, 12))\n",
    "    plt.title('Top 30 Características Más Importantes - Modelo Final XGBoost')\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.show()\n",
    "    print(\"\\nTop 30 Características:\")\n",
    "    print(feature_importances.nlargest(30))\n",
    "else:\n",
    "    print(\"X_train no es un DataFrame de Pandas, no se pueden mostrar nombres de features directamente.\")\n",
    "    # Si guardaste 'columnas_procesadas_finales.json', cárgalo aquí para los nombres\n",
    "    # print(modelo_final_xgb.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4af5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== PASO FINAL: Generando y Evaluando Predicciones para el Conjunto de TEST OFICIAL ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "RUTA_DATOS_PROCESADOS = './datos/output/procesado_para_modelado/'\n",
    "ARCHIVO_DIM_TIENDA_TEST = './datos/DIM_TIENDA_TEST.csv'\n",
    "\n",
    "# --- Cargar el X_test_oficial_procesado y los nombres de columnas ---\n",
    "try:\n",
    "    print(f\"Cargando X_test_oficial_procesado desde: {RUTA_DATOS_PROCESADOS}\")\n",
    "    # (Lógica de carga de X_test_oficial_cargado como en la respuesta anterior,\n",
    "    #  ya sea desde CSV + JSON de columnas, o NPY + JSON de columnas)\n",
    "    # ...\n",
    "    # Suponemos que al final tienes X_test_oficial_cargado (DataFrame de Pandas)\n",
    "    # ...\n",
    "    with open(RUTA_DATOS_PROCESADOS + 'columnas_procesadas_finales.json', 'r') as f: # Asegurar que tenemos los nombres\n",
    "        nombres_columnas_procesadas_test = json.load(f)\n",
    "    try:\n",
    "        X_test_oficial_cargado = pd.read_csv(RUTA_DATOS_PROCESADOS + 'X_test_oficial_procesado.csv')\n",
    "        # Asegurar orden y columnas\n",
    "        missing_cols_in_test_csv = set(nombres_columnas_procesadas_test) - set(X_test_oficial_cargado.columns)\n",
    "        if missing_cols_in_test_csv: raise ValueError(f\"Faltan columnas en X_test_oficial_procesado.csv: {missing_cols_in_test_csv}\")\n",
    "        extra_cols_in_test_csv = set(X_test_oficial_cargado.columns) - set(nombres_columnas_procesadas_test)\n",
    "        if extra_cols_in_test_csv: X_test_oficial_cargado = X_test_oficial_cargado.drop(columns=list(extra_cols_in_test_csv))\n",
    "        X_test_oficial_cargado = X_test_oficial_cargado[nombres_columnas_procesadas_test]\n",
    "    except FileNotFoundError:\n",
    "        X_test_oficial_arr_cargado = np.load(RUTA_DATOS_PROCESADOS + 'X_test_oficial_procesado.npy', allow_pickle=True)\n",
    "        X_test_oficial_cargado = pd.DataFrame(X_test_oficial_arr_cargado, columns=nombres_columnas_procesadas_test)\n",
    "    print(f\"X_test_oficial_procesado cargado. Shape: {X_test_oficial_cargado.shape}\")\n",
    "\n",
    "\n",
    "    # --- Cargar el y_test_oficial_real (el target real para DIM_TIENDA_TEST) ---\n",
    "    ARCHIVO_Y_TEST_OFICIAL_REAL = RUTA_DATOS_PROCESADOS + 'y_test_oficial_real.csv'\n",
    "    y_test_oficial_real_df = pd.read_csv(ARCHIVO_Y_TEST_OFICIAL_REAL)\n",
    "    # Necesitamos alinear y_test_oficial_real_df con el orden de X_test_oficial_cargado\n",
    "    # Para ello, primero cargamos los TIENDA_ID originales del DIM_TIENDA_TEST.csv\n",
    "    df_test_original_para_ids = pd.read_csv(ARCHIVO_DIM_TIENDA_TEST) # Usamos el original de `./datos/`\n",
    "    \n",
    "    # Crear un DataFrame temporal con TIENDA_ID para el X_test_oficial (que ya está procesado y ordenado)\n",
    "    # Necesitamos una forma de mapear el índice de X_test_oficial_cargado a los TIENDA_ID.\n",
    "    # Si X_test_oficial_cargado mantuvo el índice original de df_test_oficial_gdf (del notebook 07b),\n",
    "    # y ese índice original de df_test_oficial_gdf es el mismo que el de df_test_original_para_ids.\n",
    "    # La forma más segura es: X_test_oficial_cargado debería tener el mismo número de filas que df_test_original_para_ids\n",
    "    if len(df_test_original_para_ids) != len(X_test_oficial_cargado):\n",
    "        raise ValueError(\"El número de filas en DIM_TIENDA_TEST.csv original no coincide con X_test_oficial_cargado.\")\n",
    "\n",
    "    # Unir y_test_oficial_real_df con los TIENDA_ID en el orden de X_test_oficial_cargado\n",
    "    # Asumimos que df_test_original_para_ids tiene el orden correcto y X_test_oficial_cargado también\n",
    "    temp_df_test_ids = pd.DataFrame({'TIENDA_ID': df_test_original_para_ids['TIENDA_ID']})\n",
    "    temp_df_test_con_y_real = temp_df_test_ids.merge(y_test_oficial_real_df, on='TIENDA_ID', how='left')\n",
    "    \n",
    "    if temp_df_test_con_y_real['EXITO_REAL'].isnull().any():\n",
    "        print(\"ADVERTENCIA: Algunos TIENDA_ID del test oficial no tienen un EXITO_REAL correspondiente.\")\n",
    "        # Decide cómo manejar esto: eliminar esas filas de X_test_oficial_cargado y temp_df_test_con_y_real,\n",
    "        # o investigar por qué faltan. Por ahora, se eliminarán para la evaluación.\n",
    "        indices_con_y_real = temp_df_test_con_y_real.dropna(subset=['EXITO_REAL']).index\n",
    "        X_test_oficial_cargado = X_test_oficial_cargado.iloc[indices_con_y_real]\n",
    "        y_test_oficial_real_final = temp_df_test_con_y_real['EXITO_REAL'].iloc[indices_con_y_real].values\n",
    "        df_ids_para_pred_final = temp_df_test_ids.iloc[indices_con_y_real]['TIENDA_ID'].values\n",
    "        print(f\"Evaluación se realizará sobre {len(y_test_oficial_real_final)} tiendas de test con target real.\")\n",
    "    else:\n",
    "        y_test_oficial_real_final = temp_df_test_con_y_real['EXITO_REAL'].values\n",
    "        df_ids_para_pred_final = temp_df_test_ids['TIENDA_ID'].values\n",
    "\n",
    "    print(f\"Target real y_test_oficial_real cargado y alineado: {len(y_test_oficial_real_final)} etiquetas.\")\n",
    "\n",
    "\n",
    "    # --- Cargar el modelo final entrenado ---\n",
    "    modelo_final_cargado = joblib.load(RUTA_DATOS_PROCESADOS + 'modelo_final_xgb.joblib')\n",
    "    print(\"Modelo final XGBoost cargado.\")\n",
    "\n",
    "    # --- Generar Predicciones en X_test_oficial_cargado ---\n",
    "    print(\"\\nGenerando predicciones en X_test_oficial_cargado...\")\n",
    "    predicciones_test_oficial = modelo_final_cargado.predict(X_test_oficial_cargado)\n",
    "    predicciones_proba_test_oficial = modelo_final_cargado.predict_proba(X_test_oficial_cargado)[:, 1]\n",
    "    print(\"Predicciones generadas.\")\n",
    "\n",
    "    # --- Evaluar el Modelo en el Conjunto de TEST OFICIAL ---\n",
    "    # Usar la función evaluar_modelo definida en el notebook 09_...\n",
    "    # Asegúrate de que la función evaluar_modelo esté disponible aquí (copia su definición o impórtala)\n",
    "    # def evaluar_modelo(...): ... (definición de la función)\n",
    "    print(\"\\n--- Evaluación del Modelo Final en Conjunto de TEST OFICIAL (DIM_TIENDA_TEST.csv) ---\")\n",
    "    res_test_oficial = evaluar_modelo(\n",
    "        \"Modelo Final XGBoost (en Test Oficial)\", \n",
    "        y_test_oficial_real_final, \n",
    "        predicciones_test_oficial, \n",
    "        predicciones_proba_test_oficial\n",
    "    )\n",
    "    # Aquí podrías añadir res_test_oficial a una lista si quieres comparar con el rendimiento en validación.\n",
    "    if res_test_oficial[\"accuracy\"] > 0.80:\n",
    "        print(f\"\\n¡FELICIDADES! El modelo cumple el objetivo de asertividad (>80%) en el Test Oficial: {res_test_oficial['accuracy']:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nEl modelo NO cumple el objetivo de asertividad (>80%) en el Test Oficial: {res_test_oficial['accuracy']:.4f}\")\n",
    "\n",
    "\n",
    "    # --- Guardar las predicciones junto con el target real y los TIENDA_ID ---\n",
    "    df_predicciones_salida = pd.DataFrame({\n",
    "        'TIENDA_ID': df_ids_para_pred_final,\n",
    "        'EXITO_REAL': y_test_oficial_real_final,\n",
    "        'PREDICCION_EXITO': predicciones_test_oficial,\n",
    "        'PROBABILIDAD_EXITO_1': predicciones_proba_test_oficial\n",
    "    })\n",
    "    \n",
    "    df_predicciones_salida.to_csv(RUTA_DATOS_PROCESADOS + 'predicciones_evaluacion_DIM_TIENDA_TEST_OFICIAL.csv', index=False)\n",
    "    print(f\"\\nPredicciones y target real para DIM_TIENDA_TEST guardadas en '{RUTA_DATOS_PROCESADOS}predicciones_evaluacion_DIM_TIENDA_TEST_OFICIAL.csv'\")\n",
    "    print(df_predicciones_salida.head())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Archivo no encontrado. Asegúrate de que los datos procesados, el modelo y el target de test real existan. Detalle: {e}\")\n",
    "    print(\"Verifica que 'y_test_oficial_real.csv' se haya creado y esté en la ruta correcta.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generando/evaluando predicciones para el Test Oficial: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n--- Proceso de Generación y Evaluación de Predicciones para Test Oficial Completado ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad4928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# === MODELO ADICIONAL 1: XGBoost Refinado (Orig+ScalePW, Foco Clase 0) en TEST OFICIAL ===\n",
    "# ============================================================================================\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"=== MODELO ADICIONAL 1: XGBoost Refinado (Orig+ScalePW) - Evaluación en Test Oficial ===\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "params_xgb_refined_orig = {\n",
    "    'subsample': 1.0, \n",
    "    'reg_lambda': 5, \n",
    "    'reg_alpha': 0, \n",
    "    'n_estimators': 200, \n",
    "    'max_depth': 6, \n",
    "    'learning_rate': 0.01, \n",
    "    'gamma': 0.1, \n",
    "    'colsample_bytree': 1.0\n",
    "}\n",
    "\n",
    "# y_train debe estar disponible desde el entrenamiento del modelo principal\n",
    "if 'y_train' not in locals() or y_train is None:\n",
    "    raise NameError(\"La variable 'y_train' no está definida. Asegúrate de que los datos de entrenamiento estén cargados.\")\n",
    "\n",
    "count_neg_ref_orig, count_pos_ref_orig = y_train.value_counts().sort_index()\n",
    "scale_pos_w_ref_orig = count_neg_ref_orig / count_pos_ref_orig\n",
    "print(f\"Usando scale_pos_weight para XGBoost Refinado (Orig): {scale_pos_w_ref_orig:.4f}\")\n",
    "\n",
    "modelo_xgb_refinado_orig_add = xgb.XGBClassifier( # Nuevo nombre para no sobrescribir\n",
    "    **params_xgb_refined_orig,\n",
    "    scale_pos_weight=scale_pos_w_ref_orig,\n",
    "    random_state=RANDOM_STATE,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando XGBoost Refinado (Orig+ScalePW) con X_train y y_train...\")\n",
    "try:\n",
    "    start_time = time()\n",
    "    modelo_xgb_refinado_orig_add.fit(X_train, y_train) # X_train y y_train deben estar disponibles\n",
    "    end_time = time()\n",
    "    print(f\"Modelo XGBoost Refinado (Orig+ScalePW) entrenado en {end_time - start_time:.2f} segundos.\")\n",
    "\n",
    "    joblib.dump(modelo_xgb_refinado_orig_add, RUTA_DATOS_PROCESADOS + 'modelo_xgb_refinado_orig_clase0_ADD.joblib')\n",
    "    print(f\"Modelo XGBoost Refinado (Orig+ScalePW) guardado como 'modelo_xgb_refinado_orig_clase0_ADD.joblib'.\")\n",
    "\n",
    "    # Predicciones en el Test Oficial usando X_test_oficial_cargado y y_test_oficial_real_final\n",
    "    # Estas variables DEBEN estar definidas por la evaluación de tu modelo principal ANTERIORMENTE en este notebook.\n",
    "    if 'X_test_oficial_cargado' not in locals() or 'y_test_oficial_real_final' not in locals():\n",
    "        raise NameError(\"X_test_oficial_cargado o y_test_oficial_real_final no están definidos. Ejecuta la evaluación del modelo principal primero.\")\n",
    "\n",
    "    print(\"\\nGenerando predicciones en el Test Oficial con XGBoost Refinado (Orig+ScalePW)...\")\n",
    "    predicciones_test_xgb_ref_orig_add = modelo_xgb_refinado_orig_add.predict(X_test_oficial_cargado)\n",
    "\n",
    "    print(\"\\n--- Matriz de Confusión: XGBoost Refinado (Orig+ScalePW) en Test Oficial ---\")\n",
    "    cm_xgb_ref_orig_add = confusion_matrix(y_test_oficial_real_final, predicciones_test_xgb_ref_orig_add)\n",
    "    sns.heatmap(cm_xgb_ref_orig_add, annot=True, fmt='d', cmap='Greens')\n",
    "    plt.title('CM - XGBoost Refinado (Orig+ScalePW) en Test Oficial')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nClassification Report (XGBoost Refinado Orig+ScalePW en Test Oficial):\")\n",
    "    print(classification_report(y_test_oficial_real_final, predicciones_test_xgb_ref_orig_add, zero_division=0))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error con XGBoost Refinado (Orig+ScalePW): {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
