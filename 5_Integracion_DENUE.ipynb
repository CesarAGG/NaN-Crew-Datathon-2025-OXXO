{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca64a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import polars as pl # Para cargar DENUE si es muy grande\n",
    "import numpy as np\n",
    "import json # Para cargar el diccionario de categorías SCIAN\n",
    "from shapely.geometry import Point\n",
    "from tqdm.auto import tqdm \n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bb9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rutas y Nombres de Archivo (Configurables) ---\n",
    "RUTA_TIENDAS_CON_CENSO_INPUT = './datos/output/tiendas_train_final_con_censo_ageb.gpkg'\n",
    "LAYER_TIENDAS_CON_CENSO = 'tiendas_censo_ageb_nl_tam' # Capa dentro del gpkg\n",
    "\n",
    "# Archivos del DENUE (ajusta si es uno nacional o varios por estado)\n",
    "RUTA_DENUE_RAW = './datos/denue_raw/' \n",
    "ARCHIVOS_DENUE_CSV = [\n",
    "    RUTA_DENUE_RAW + 'denue_nuevo_leon.csv', # Ejemplo\n",
    "    RUTA_DENUE_RAW + 'denue_tamaulipas.csv'  # Ejemplo\n",
    "]\n",
    "# Si es un solo archivo nacional:\n",
    "# ARCHIVOS_DENUE_CSV = [RUTA_DENUE_RAW + 'denue_nacional_completo.csv'] \n",
    "\n",
    "# Nombres de columnas en tus archivos DENUE (¡VERIFICA ESTOS!)\n",
    "COL_DENUE_LATITUD = 'latitud'\n",
    "COL_DENUE_LONGITUD = 'longitud'\n",
    "COL_DENUE_SCIAN = 'codigo_act' # o 'id_actividad', etc.\n",
    "COL_DENUE_ENTIDAD_PARA_FILTRAR = 'entidad' # Si descargas nacional y necesitas filtrar por CVE_ENT de NL (19) y TAM (28)\n",
    "\n",
    "# Archivo JSON con las categorías PDI y códigos SCIAN\n",
    "RUTA_CATEGORIAS_SCIAN_JSON = './datos/output/categorias_pdi_scian_final.json'\n",
    "\n",
    "# CRS de las tiendas (EPSG:6372, ya proyectado) y CRS original de DENUE (WGS84)\n",
    "CRS_OBJETIVO = 'EPSG:6372'\n",
    "CRS_DENUE_ORIGINAL = 'EPSG:4326' \n",
    "\n",
    "# Radios para análisis de conteo (en metros)\n",
    "RADIOS_ANALISIS_M = [200, 500, 1000] \n",
    "\n",
    "# Archivo de salida\n",
    "RUTA_DATOS_OUTPUT = './datos/output/'\n",
    "ARCHIVO_SALIDA_GPKG = RUTA_DATOS_OUTPUT + 'tiendas_train_censo_denue.gpkg'\n",
    "# --- Fin de Configuración ---\n",
    "\n",
    "print(\"--- Iniciando Integración de Datos del DENUE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e467e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Cargar el GeoDataFrame de Tiendas (ya con Censo) ---\n",
    "try:\n",
    "    tiendas_gdf = gpd.read_file(RUTA_TIENDAS_CON_CENSO_INPUT, layer=LAYER_TIENDAS_CON_CENSO)\n",
    "    print(f\"GeoDataFrame de tiendas con censo cargado: {len(tiendas_gdf)} tiendas.\")\n",
    "    # Asegurar que el CRS es el correcto, si no, reproyectar.\n",
    "    if tiendas_gdf.crs is None or tiendas_gdf.crs.to_string().upper() != CRS_OBJETIVO.upper():\n",
    "        print(f\"CRS de tiendas_gdf ({tiendas_gdf.crs}) no es el esperado ({CRS_OBJETIVO}). Reproyectando...\")\n",
    "        tiendas_gdf = tiendas_gdf.to_crs(CRS_OBJETIVO)\n",
    "    print(f\"CRS de tiendas_gdf: {tiendas_gdf.crs}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar '{RUTA_TIENDAS_CON_CENSO_INPUT}': {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246059a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Cargar el Diccionario de Categorías PDI SCIAN ---\n",
    "try:\n",
    "    with open(RUTA_CATEGORIAS_SCIAN_JSON, 'r', encoding='utf-8') as f:\n",
    "        categorias_pdi_scian = json.load(f)\n",
    "    print(f\"\\nDiccionario de categorías PDI SCIAN cargado desde '{RUTA_CATEGORIAS_SCIAN_JSON}'.\")\n",
    "    # print(categorias_pdi_scian)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Archivo JSON '{RUTA_CATEGORIAS_SCIAN_JSON}' no encontrado.\")\n",
    "    print(\"Asegúrate de haber ejecutado el notebook '00_Procesar_Catalogo_SCIAN_Avanzado.ipynb' y guardado el JSON.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el archivo JSON de categorías SCIAN: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Cargar y Preparar Datos del DENUE ---\n",
    "denue_dfs_list = []\n",
    "print(\"\\nCargando datos del DENUE...\")\n",
    "for archivo_csv in ARCHIVOS_DENUE_CSV:\n",
    "    try:\n",
    "        # Para archivos DENUE muy grandes, scan_csv de Polars es preferible\n",
    "        # Aquí usamos Pandas por simplicidad, asumiendo que los archivos por estado son manejables\n",
    "        # O que el filtrado por estado se hace primero si es un archivo nacional muy grande.\n",
    "        df_temp = pd.read_csv(archivo_csv, low_memory=False, encoding='latin1') # DENUE suele venir en latin1\n",
    "        # Si es un archivo nacional, filtrar por Nuevo León (19) y Tamaulipas (28)\n",
    "        # if 'nombre_columna_clave_entidad_denue' in df_temp.columns: # Reemplaza con el nombre real\n",
    "        #     df_temp = df_temp[df_temp['nombre_columna_clave_entidad_denue'].isin([19, 28])]\n",
    "        denue_dfs_list.append(df_temp)\n",
    "        print(f\"  Cargadas {len(df_temp)} unidades de {archivo_csv} (después de posible filtro estatal).\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Error: Archivo DENUE no encontrado en '{archivo_csv}'. Omitiendo.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error al cargar '{archivo_csv}': {e}. Omitiendo.\")\n",
    "\n",
    "if not denue_dfs_list:\n",
    "    raise FileNotFoundError(\"No se pudo cargar ningún archivo del DENUE. Verifica rutas y nombres.\")\n",
    "\n",
    "denue_df_completo = pd.concat(denue_dfs_list, ignore_index=True)\n",
    "print(f\"Total de unidades DENUE cargadas (NL y TAM): {len(denue_df_completo)}\")\n",
    "\n",
    "# Verificar columnas y limpiar coordenadas\n",
    "denue_cols_necesarias = [COL_DENUE_LATITUD, COL_DENUE_LONGITUD, COL_DENUE_SCIAN]\n",
    "if not all(col in denue_df_completo.columns for col in denue_cols_necesarias):\n",
    "    print(f\"Error: Faltan columnas esenciales en DENUE. Necesarias: {denue_cols_necesarias}\")\n",
    "    print(f\"Columnas disponibles: {denue_df_completo.columns.tolist()}\")\n",
    "    raise ValueError(\"Columnas faltantes en DENUE.\")\n",
    "\n",
    "denue_df_completo[COL_DENUE_LATITUD] = pd.to_numeric(denue_df_completo[COL_DENUE_LATITUD], errors='coerce')\n",
    "denue_df_completo[COL_DENUE_LONGITUD] = pd.to_numeric(denue_df_completo[COL_DENUE_LONGITUD], errors='coerce')\n",
    "denue_df_completo.dropna(subset=[COL_DENUE_LATITUD, COL_DENUE_LONGITUD], inplace=True)\n",
    "denue_df_completo[COL_DENUE_SCIAN] = denue_df_completo[COL_DENUE_SCIAN].astype(str).str.strip() # Estandarizar SCIAN a string\n",
    "\n",
    "print(f\"Unidades DENUE después de limpiar coordenadas: {len(denue_df_completo)}\")\n",
    "\n",
    "# Convertir DENUE a GeoDataFrame y reproyectar\n",
    "try:\n",
    "    denue_gdf = gpd.GeoDataFrame(\n",
    "        denue_df_completo,\n",
    "        geometry=gpd.points_from_xy(denue_df_completo[COL_DENUE_LONGITUD], denue_df_completo[COL_DENUE_LATITUD]),\n",
    "        crs=CRS_DENUE_ORIGINAL\n",
    "    )\n",
    "    if denue_gdf.crs != tiendas_gdf.crs:\n",
    "        print(f\"Reproyectando DENUE GDF de {denue_gdf.crs} a {tiendas_gdf.crs}...\")\n",
    "        denue_gdf = denue_gdf.to_crs(tiendas_gdf.crs)\n",
    "    print(f\"GeoDataFrame del DENUE preparado. CRS: {denue_gdf.crs}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear o reproyectar GeoDataFrame del DENUE: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Filtrar DENUE por Categorías de PDI y Generar Características ---\n",
    "# Usaremos el diccionario `categorias_pdi_scian` cargado del JSON\n",
    "\n",
    "gdfs_pdi_denue_filtrados = {}\n",
    "print(\"\\nFiltrando DENUE por categorías PDI definidas...\")\n",
    "for categoria, scian_codes_list in categorias_pdi_scian.items():\n",
    "    if not scian_codes_list: # Si la lista de códigos está vacía para una categoría\n",
    "        print(f\"  Categoría '{categoria}' no tiene códigos SCIAN definidos. Omitiendo.\")\n",
    "        gdfs_pdi_denue_filtrados[categoria] = gpd.GeoDataFrame(columns=denue_gdf.columns, geometry=[], crs=denue_gdf.crs) # GDF vacío\n",
    "        continue\n",
    "    \n",
    "    # Asegurar que los códigos SCIAN a filtrar sean strings\n",
    "    scian_codes_str = [str(code).strip() for code in scian_codes_list]\n",
    "    \n",
    "    # Filtrar usando isin para códigos exactos de 6 dígitos\n",
    "    # Si quieres usar prefijos (ej. '611' para todo educación), usarías .str.startswith()\n",
    "    temp_gdf = denue_gdf[denue_gdf[COL_DENUE_SCIAN].isin(scian_codes_str)]\n",
    "    gdfs_pdi_denue_filtrados[categoria] = temp_gdf\n",
    "    print(f\"  Categoría '{categoria}': {len(temp_gdf)} establecimientos encontrados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 4.A Contar PDIs en Radios ---\n",
    "print(\"\\nCalculando conteo de PDIs del DENUE en radios alrededor de cada tienda OXXO...\")\n",
    "for radio_m in RADIOS_ANALISIS_M:\n",
    "    print(f\"  Procesando para radio de {radio_m} metros...\")\n",
    "    # Crear buffer alrededor de las tiendas OXXO una sola vez por radio\n",
    "    tiendas_buffer_gdf = tiendas_gdf.copy() # Trabajar sobre una copia para los buffers\n",
    "    tiendas_buffer_gdf['geometry_buffer'] = tiendas_buffer_gdf.geometry.buffer(radio_m)\n",
    "    # Usar la geometría del buffer para el sjoin\n",
    "    tiendas_buffer_gdf = tiendas_buffer_gdf.set_geometry('geometry_buffer')\n",
    "\n",
    "\n",
    "    for categoria, pdi_gdf_cat in tqdm(gdfs_pdi_denue_filtrados.items(), desc=f\"Cat. DENUE (radio {radio_m}m)\"):\n",
    "        nombre_col_conteo = f'denue_conteo_{categoria}_{radio_m}m'\n",
    "        if not pdi_gdf_cat.empty:\n",
    "            # Unión espacial para identificar qué PDIs caen en qué buffers de tienda\n",
    "            # sjoin_predicates=['intersects'] es el nuevo nombre para 'op' en versiones recientes de geopandas\n",
    "            # o simplemente op='intersects' para versiones más antiguas\n",
    "            try: # Intentar con la sintaxis más nueva primero\n",
    "                 joined_conteo = gpd.sjoin(tiendas_buffer_gdf[['TIENDA_ID', 'geometry_buffer']], \n",
    "                                      pdi_gdf_cat[['geometry']], \n",
    "                                      how='left', \n",
    "                                      predicate='intersects') # o 'contains' si el PDI debe estar completamente dentro\n",
    "            except TypeError: # Fallback a la sintaxis antigua\n",
    "                 joined_conteo = gpd.sjoin(tiendas_buffer_gdf[['TIENDA_ID', 'geometry_buffer']], \n",
    "                                      pdi_gdf_cat[['geometry']], \n",
    "                                      how='left', \n",
    "                                      op='intersects')\n",
    "\n",
    "            # Contar PDIs por TIENDA_ID\n",
    "            conteo_por_tienda = joined_conteo.dropna(subset=['index_right']).groupby('TIENDA_ID').size()\n",
    "            tiendas_gdf[nombre_col_conteo] = tiendas_gdf['TIENDA_ID'].map(conteo_por_tienda).fillna(0).astype(int)\n",
    "        else:\n",
    "            tiendas_gdf[nombre_col_conteo] = 0\n",
    "        # print(f\"    Columna creada: {nombre_col_conteo}, suma: {tiendas_gdf[nombre_col_conteo].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 4.B Distancia al PDI más Cercano ---\n",
    "# NOTA: Este bucle puede ser MUY LENTO. Para un datathon, considera:\n",
    "# 1. Hacerlo solo para las 1-2 categorías MÁS IMPORTANTES (ej. competidores_directos).\n",
    "# 2. Si el tiempo apremia, omitir este paso o hacerlo sobre una muestra.\n",
    "# 3. Investigar/implementar optimizaciones con rtree o cKDTree si tienes mucha experiencia y tiempo.\n",
    "\n",
    "print(\"\\nCalculando distancia al PDI del DENUE más cercano por categoría (puede tardar)...\")\n",
    "for categoria, pdi_gdf_cat in tqdm(gdfs_pdi_denue_filtrados.items(), desc=\"Cat. DENUE (distancia)\"):\n",
    "    nombre_col_dist = f'denue_dist_{categoria}_cercano_m'\n",
    "    if not pdi_gdf_cat.empty and not pdi_gdf_cat.geometry.is_empty.all() and len(pdi_gdf_cat) > 0:\n",
    "        # Crear un árbol espacial para los PDIs de esta categoría para búsquedas eficientes\n",
    "        # Esto requiere la biblioteca 'rtree' instalada: uv pip install rtree\n",
    "        try:\n",
    "            sindex_pdi_cat = pdi_gdf_cat.sindex\n",
    "            \n",
    "            distancias = []\n",
    "            for idx_tienda, tienda_row in tqdm(tiendas_gdf.iterrows(), total=len(tiendas_gdf), desc=f\"  Dist. {categoria}\", leave=False):\n",
    "                tienda_geom = tienda_row.geometry\n",
    "                # Obtener posibles candidatos cercanos usando el índice espacial\n",
    "                posibles_matches_indices = list(sindex_pdi_cat.intersection(tienda_geom.buffer(max(RADIOS_ANALISIS_M) * 2).bounds)) # Buscar en un radio amplio\n",
    "                \n",
    "                if not posibles_matches_indices:\n",
    "                    distancias.append(np.nan) # O un valor grande como 99999\n",
    "                    continue\n",
    "                \n",
    "                candidatos_cercanos_gdf = pdi_gdf_cat.iloc[posibles_matches_indices]\n",
    "                \n",
    "                if candidatos_cercanos_gdf.empty:\n",
    "                     distancias.append(np.nan)\n",
    "                     continue\n",
    "\n",
    "                # Calcular distancias solo a estos candidatos\n",
    "                min_dist = candidatos_cercanos_gdf.geometry.distance(tienda_geom).min()\n",
    "                distancias.append(min_dist)\n",
    "\n",
    "            tiendas_gdf[nombre_col_dist] = distancias\n",
    "            tiendas_gdf[nombre_col_dist] = tiendas_gdf[nombre_col_dist].fillna(99999).astype(float) # Imputar NaNs y asegurar float\n",
    "        except Exception as e_dist_optim: # Fallback a método simple si sindex falla o rtree no está\n",
    "            print(f\"    Advertencia: Falló el cálculo optimizado de distancia para {categoria} ({e_dist_optim}). Usando método simple (más lento)...\")\n",
    "            distancias_simple = tiendas_gdf.geometry.progress_apply(lambda g: pdi_gdf_cat.geometry.distance(g).min() if not pdi_gdf_cat.empty else np.nan)\n",
    "            tiendas_gdf[nombre_col_dist] = distancias_simple.fillna(99999).astype(float)\n",
    "            \n",
    "    else: # Si no hay PDIs en la categoría\n",
    "        tiendas_gdf[nombre_col_dist] = 99999.0 \n",
    "    \n",
    "    mean_dist_val = tiendas_gdf[nombre_col_dist][tiendas_gdf[nombre_col_dist] < 99999].mean()\n",
    "    print(f\"  Columna creada: {nombre_col_dist}, promedio dist (excl. 99999): {mean_dist_val:.2f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848070e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Guardar el Resultado ---\n",
    "print(\"\\n--- Guardando GeoDataFrame Enriquecido con DENUE ---\")\n",
    "# Asegurar que la columna de geometría original de las tiendas se mantenga si es necesario\n",
    "# El sjoin de conteo con buffer podría haberla modificado si no se manejó con cuidado\n",
    "# tiendas_gdf = tiendas_gdf.set_geometry('geometry') # Asegura que 'geometry' es la columna activa\n",
    "\n",
    "print(\"Primeras filas del dataset final con datos DENUE:\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(tiendas_gdf.head())\n",
    "print(\"\\nColumnas finales:\")\n",
    "print(tiendas_gdf.columns.tolist())\n",
    "\n",
    "try:\n",
    "    tiendas_gdf.to_file(ARCHIVO_SALIDA_GPKG, driver='GPKG', layer='tiendas_censo_denue')\n",
    "    print(f\"\\nDataset enriquecido guardado como GeoPackage en '{ARCHIVO_SALIDA_GPKG}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al guardar el archivo de salida GeoPackage: {e}\")\n",
    "\n",
    "print(\"\\n--- Integración de Datos del DENUE Completada ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
