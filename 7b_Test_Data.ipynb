{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73baa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import json\n",
    "from shapely.geometry import Point\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb406a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración de Rutas y Nombres de Archivo (Globales para este Notebook) ---\n",
    "# Directorios\n",
    "RUTA_DATOS_INPUT_ORIGINALES = './datos/'\n",
    "RUTA_DATOS_CENSO_CSV_DIR = './datos/censo/'\n",
    "RUTA_MGN_SHAPEFILES_DIR = './datos/mgn/conjunto_de_datos/'\n",
    "RUTA_DENUE_RAW_DIR = './datos/denue_raw/'\n",
    "RUTA_OSM_DATA_DIR = './datos/osm_data/'\n",
    "RUTA_IMU_CSV_DIR = './datos/imu_data/'\n",
    "RUTA_DATOS_OUTPUT_GENERAL = './datos/output/' # Para el JSON de SCIAN y el resultado final\n",
    "\n",
    "# Archivo de entrada principal para este notebook\n",
    "ARCHIVO_DIM_TIENDA_TEST = RUTA_DATOS_INPUT_ORIGINALES + 'DIM_TIENDA_TEST.csv'\n",
    "\n",
    "# Archivos de datos externos (asumiendo nombres y ubicaciones de scripts anteriores)\n",
    "# Censo\n",
    "ARCHIVO_CENSO_NL = RUTA_DATOS_CENSO_CSV_DIR + 'nuevo_leon_censo_ageb.csv'\n",
    "ARCHIVO_CENSO_TAM = RUTA_DATOS_CENSO_CSV_DIR + 'tamaulipas_censo_ageb.csv'\n",
    "COL_CENSO_SCITEL_ENTIDAD = 'ENTIDAD' # Ajustar si es diferente en tus CSV de SCITEL\n",
    "COL_CENSO_SCITEL_MUNICIPIO = 'MUN'\n",
    "COL_CENSO_SCITEL_LOCALIDAD = 'LOC'\n",
    "COL_CENSO_SCITEL_AGEB = 'AGEB'\n",
    "COL_CENSO_CVEGEO_CONSTRUIDA = 'CVEGEO_CONSTRUIDO'\n",
    "\n",
    "# MGN (AGEBs)\n",
    "SHAPEFILE_AGEB_NACIONAL = RUTA_MGN_SHAPEFILES_DIR + '00a.shp' \n",
    "COL_SHP_CVEGEO = 'CVEGEO' \n",
    "CRS_SHAPEFILE_AGEB = 'EPSG:6372'\n",
    "\n",
    "# DENUE\n",
    "ARCHIVOS_DENUE_CSV_LISTA = [\n",
    "    RUTA_DENUE_RAW_DIR + 'denue_nuevo_leon.csv', \n",
    "    RUTA_DENUE_RAW_DIR + 'denue_tamaulipas.csv'\n",
    "]\n",
    "COL_DENUE_LAT = 'latitud'\n",
    "COL_DENUE_LON = 'longitud'\n",
    "COL_DENUE_SCIAN_CODE = 'codigo_act'\n",
    "JSON_CATEGORIAS_SCIAN = RUTA_DATOS_OUTPUT_GENERAL + 'categorias_pdi_scian_final.json'\n",
    "\n",
    "# OSM\n",
    "SHP_OSM_POIS_FILE = RUTA_OSM_DATA_DIR + 'gis_osm_pois_a_free_1.shp' \n",
    "SHP_OSM_ROADS_FILE = RUTA_OSM_DATA_DIR + 'gis_osm_roads_free_1.shp'\n",
    "COL_OSM_POI_FCLASS = 'fclass'\n",
    "COL_OSM_ROAD_FCLASS = 'fclass'\n",
    "\n",
    "# IMU\n",
    "ARCHIVO_IMU_CSV = RUTA_IMU_CSV_DIR + 'indice_marginacion_urbana_ageb_2020.csv'\n",
    "COL_IMU_CVEGEO_KEY = 'CVE_AGEB' # Clave en el CSV del IMU\n",
    "COL_IMU_INDEX_VALUE = 'IM_2020'\n",
    "COL_IMU_GRADE_VALUE = 'GM_2020'\n",
    "COL_IMU_NAT_INDEX_VALUE = 'IMN_2020'\n",
    "\n",
    "# Parámetros de Análisis\n",
    "RADIOS_M = [200, 500, 1000] # Radios para conteos de PDIs\n",
    "\n",
    "# Archivo de Salida para este notebook\n",
    "ARCHIVO_SALIDA_TEST_ENRIQUECIDO_GPKG = RUTA_DATOS_OUTPUT_GENERAL + 'tiendas_TEST_OFICIAL_enriquecido.gpkg'\n",
    "LAYER_SALIDA_TEST_ENRIQUECIDO = 'tiendas_test_oficial_enriquecido'\n",
    "\n",
    "# CRS Globales\n",
    "CRS_PROYECTADO_OBJETIVO = CRS_SHAPEFILE_AGEB # EPSG:6372\n",
    "CRS_GEOGRAFICO_WGS84 = 'EPSG:4326'\n",
    "\n",
    "# Crear directorios de output si no existen\n",
    "if not os.path.exists(RUTA_DATOS_OUTPUT_GENERAL): os.makedirs(RUTA_DATOS_OUTPUT_GENERAL)\n",
    "# --- Fin de Configuración ---\n",
    "\n",
    "print(\"--- Iniciando Enriquecimiento de DIM_TIENDA_TEST.csv ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PASO 0: Cargar DIM_TIENDA_TEST.csv y convertir a GeoDataFrame ---\n",
    "try:\n",
    "    df_test_original = pd.read_csv(ARCHIVO_DIM_TIENDA_TEST)\n",
    "    print(f\"Cargado '{ARCHIVO_DIM_TIENDA_TEST}': {len(df_test_original)} tiendas de test.\")\n",
    "    \n",
    "    # Verificar columnas de coordenadas\n",
    "    if not {'LATITUD_NUM', 'LONGITUD_NUM'}.issubset(df_test_original.columns):\n",
    "        raise ValueError(\"Columnas 'LATITUD_NUM' y 'LONGITUD_NUM' no encontradas en DIM_TIENDA_TEST.csv\")\n",
    "\n",
    "    tiendas_test_gdf = gpd.GeoDataFrame(\n",
    "        df_test_original,\n",
    "        geometry=gpd.points_from_xy(df_test_original.LONGITUD_NUM, df_test_original.LATITUD_NUM),\n",
    "        crs=CRS_GEOGRAFICO_WGS84 \n",
    "    )\n",
    "    # Reproyectar al CRS objetivo\n",
    "    tiendas_test_gdf = tiendas_test_gdf.to_crs(CRS_PROYECTADO_OBJETIVO)\n",
    "    print(f\"GeoDataFrame de tiendas TEST creado y reproyectado a {tiendas_test_gdf.crs}. Shape: {tiendas_test_gdf.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontró '{ARCHIVO_DIM_TIENDA_TEST}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar o convertir DIM_TIENDA_TEST.csv: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PASO 1: Integración de Datos del Censo (similar a 04_...) ===\n",
    "print(\"\\n--- Iniciando Integración de Datos del Censo para TEST ---\")\n",
    "# 1.1 Cargar y concatenar CSVs del censo de NL y TAM\n",
    "try:\n",
    "    censo_nl_df = pd.read_csv(ARCHIVO_CENSO_NL, dtype={COL_CENSO_SCITEL_ENTIDAD:str, COL_CENSO_SCITEL_MUNICIPIO:str, COL_CENSO_SCITEL_LOCALIDAD:str, COL_CENSO_SCITEL_AGEB:str})\n",
    "    censo_tam_df = pd.read_csv(ARCHIVO_CENSO_TAM, dtype={COL_CENSO_SCITEL_ENTIDAD:str, COL_CENSO_SCITEL_MUNICIPIO:str, COL_CENSO_SCITEL_LOCALIDAD:str, COL_CENSO_SCITEL_AGEB:str})\n",
    "    censo_ageb_df_completo = pd.concat([censo_nl_df, censo_tam_df], ignore_index=True)\n",
    "    print(f\"Datos censales de NL y TAM cargados y concatenados: {len(censo_ageb_df_completo)} registros AGEB.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar/concatenar datos del censo: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Construir CVEGEO en datos del censo\n",
    "try:\n",
    "    censo_ageb_df_completo[COL_CENSO_SCITEL_ENTIDAD] = censo_ageb_df_completo[COL_CENSO_SCITEL_ENTIDAD].str.zfill(2)\n",
    "    censo_ageb_df_completo[COL_CENSO_SCITEL_MUNICIPIO] = censo_ageb_df_completo[COL_CENSO_SCITEL_MUNICIPIO].str.zfill(3)\n",
    "    censo_ageb_df_completo[COL_CENSO_SCITEL_LOCALIDAD] = censo_ageb_df_completo[COL_CENSO_SCITEL_LOCALIDAD].str.zfill(4)\n",
    "    censo_ageb_df_completo[COL_CENSO_SCITEL_AGEB] = censo_ageb_df_completo[COL_CENSO_SCITEL_AGEB].astype(str).str.upper().str.strip() # .str.pad(width=4, side='left', fillchar='0')\n",
    "\n",
    "    censo_ageb_df_completo[COL_CENSO_CVEGEO_CONSTRUIDA] = (\n",
    "        censo_ageb_df_completo[COL_CENSO_SCITEL_ENTIDAD] +\n",
    "        censo_ageb_df_completo[COL_CENSO_SCITEL_MUNICIPIO] +\n",
    "        censo_ageb_df_completo[COL_CENSO_SCITEL_LOCALIDAD] +\n",
    "        censo_ageb_df_completo[COL_CENSO_SCITEL_AGEB]\n",
    "    )\n",
    "    print(f\"Columna '{COL_CENSO_CVEGEO_CONSTRUIDA}' creada en datos del censo.\")\n",
    "    censo_para_merge_df = censo_ageb_df_completo.drop_duplicates(subset=[COL_CENSO_CVEGEO_CONSTRUIDA])\n",
    "except Exception as e:\n",
    "    print(f\"Error construyendo CVEGEO en datos del censo: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fabeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Cargar shapefile nacional de AGEBs\n",
    "try:\n",
    "    ageb_gdf_nacional_shp = gpd.read_file(SHAPEFILE_AGEB_NACIONAL)\n",
    "    if ageb_gdf_nacional_shp.crs is None: ageb_gdf_nacional_shp.set_crs(CRS_SHAPEFILE_AGEB, inplace=True)\n",
    "    elif ageb_gdf_nacional_shp.crs.to_string().upper() != CRS_SHAPEFILE_AGEB.upper():\n",
    "        ageb_gdf_nacional_shp = ageb_gdf_nacional_shp.to_crs(CRS_SHAPEFILE_AGEB)\n",
    "    print(f\"Shapefile nacional de AGEBs cargado y con CRS {ageb_gdf_nacional_shp.crs}.\")\n",
    "    ageb_gdf_nacional_shp[COL_SHP_CVEGEO] = ageb_gdf_nacional_shp[COL_SHP_CVEGEO].astype(str).str.strip()\n",
    "except Exception as e:\n",
    "    print(f\"Error cargando shapefile nacional de AGEBs: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e8591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Unir censo a shapefile de AGEBs\n",
    "try:\n",
    "    ageb_con_censo_gdf_test = ageb_gdf_nacional_shp.merge(\n",
    "        censo_para_merge_df, # Ya tiene solo las columnas necesarias y CVEGEO construido\n",
    "        left_on=COL_SHP_CVEGEO,\n",
    "        right_on=COL_CENSO_CVEGEO_CONSTRUIDA,\n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"Censo unido a AGEBs nacionales. Shape resultante: {ageb_con_censo_gdf_test.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uniendo censo a shapefile de AGEBs: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea64a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Unión espacial de tiendas TEST con AGEBs+Censo\n",
    "try:\n",
    "    # Seleccionar columnas relevantes del censo para añadir a las tiendas\n",
    "    cols_censo_en_ageb = [col for col in censo_para_merge_df.columns if col != COL_CENSO_CVEGEO_CONSTRUIDA]\n",
    "    cols_para_sjoin_desde_ageb = [COL_SHP_CVEGEO] + cols_censo_en_ageb + ['geometry']\n",
    "    cols_para_sjoin_desde_ageb_existentes = [c for c in cols_para_sjoin_desde_ageb if c in ageb_con_censo_gdf_test.columns]\n",
    "    cols_para_sjoin_desde_ageb_existentes = list(dict.fromkeys(cols_para_sjoin_desde_ageb_existentes))\n",
    "    \n",
    "    ageb_para_sjoin_gdf_test = ageb_con_censo_gdf_test[cols_para_sjoin_desde_ageb_existentes]\n",
    "\n",
    "    tiendas_test_gdf = gpd.sjoin(\n",
    "        tiendas_test_gdf,\n",
    "        ageb_para_sjoin_gdf_test,\n",
    "        how='left',\n",
    "        predicate='within' # o predicate='within'\n",
    "    )\n",
    "    if 'index_right' in tiendas_test_gdf.columns:\n",
    "        tiendas_test_gdf = tiendas_test_gdf.drop(columns=['index_right'])\n",
    "    print(f\"Tiendas TEST unidas espacialmente con AGEBs+Censo. Shape: {tiendas_test_gdf.shape}\")\n",
    "    # Renombrar la columna CVEGEO del shapefile a 'CVEGEO' si se llamó diferente tras el sjoin\n",
    "    if COL_SHP_CVEGEO + \"_left\" in tiendas_test_gdf.columns and COL_SHP_CVEGEO not in tiendas_test_gdf.columns :\n",
    "        tiendas_test_gdf.rename(columns={COL_SHP_CVEGEO + \"_left\": \"CVEGEO\"}, inplace=True)\n",
    "    elif COL_SHP_CVEGEO + \"_right\" in tiendas_test_gdf.columns and COL_SHP_CVEGEO not in tiendas_test_gdf.columns:\n",
    "         tiendas_test_gdf.rename(columns={COL_SHP_CVEGEO + \"_right\": \"CVEGEO\"}, inplace=True)\n",
    "    elif COL_SHP_CVEGEO not in tiendas_test_gdf.columns and COL_CENSO_CVEGEO_CONSTRUIDA in tiendas_test_gdf.columns: # Si tomó la del censo\n",
    "        tiendas_test_gdf.rename(columns={COL_CENSO_CVEGEO_CONSTRUIDA: \"CVEGEO\"}, inplace=True)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error en unión espacial de tiendas TEST con AGEBs+Censo: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PASO 2: Integración de Datos del DENUE (similar a 05_...) ===\n",
    "print(\"\\n--- Iniciando Integración de Datos del DENUE para TEST ---\")\n",
    "# 2.1 Cargar diccionario SCIAN\n",
    "try:\n",
    "    with open(JSON_CATEGORIAS_SCIAN, 'r', encoding='utf-8') as f:\n",
    "        categorias_pdi_scian_dict = json.load(f)\n",
    "    print(\"Diccionario de categorías SCIAN cargado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error cargando JSON de categorías SCIAN: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea472eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Cargar y preparar DENUE GDF\n",
    "denue_dfs_list_test = []\n",
    "for archivo_csv in ARCHIVOS_DENUE_CSV_LISTA:\n",
    "    try:\n",
    "        df_temp = pd.read_csv(archivo_csv, low_memory=False, encoding='latin1')\n",
    "        denue_dfs_list_test.append(df_temp)\n",
    "    except Exception as e: print(f\"Error cargando DENUE CSV '{archivo_csv}': {e}\")\n",
    "if not denue_dfs_list_test: raise FileNotFoundError(\"No se cargaron archivos DENUE para TEST.\")\n",
    "denue_df_completo_test = pd.concat(denue_dfs_list_test, ignore_index=True)\n",
    "\n",
    "denue_df_completo_test[COL_DENUE_LAT] = pd.to_numeric(denue_df_completo_test[COL_DENUE_LAT], errors='coerce')\n",
    "denue_df_completo_test[COL_DENUE_LON] = pd.to_numeric(denue_df_completo_test[COL_DENUE_LON], errors='coerce')\n",
    "denue_df_completo_test.dropna(subset=[COL_DENUE_LAT, COL_DENUE_LON], inplace=True)\n",
    "denue_df_completo_test[COL_DENUE_SCIAN_CODE] = denue_df_completo_test[COL_DENUE_SCIAN_CODE].astype(str).str.strip()\n",
    "\n",
    "denue_gdf_test = gpd.GeoDataFrame(\n",
    "    denue_df_completo_test,\n",
    "    geometry=gpd.points_from_xy(denue_df_completo_test[COL_DENUE_LON], denue_df_completo_test[COL_DENUE_LAT]),\n",
    "    crs=CRS_GEOGRAFICO_WGS84\n",
    ")\n",
    "if denue_gdf_test.crs.to_string().upper() != CRS_PROYECTADO_OBJETIVO.upper():\n",
    "    denue_gdf_test = denue_gdf_test.to_crs(CRS_PROYECTADO_OBJETIVO)\n",
    "print(f\"DENUE para TEST preparado y reproyectado. Shape: {denue_gdf_test.shape}, CRS: {denue_gdf_test.crs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcb49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Filtrar DENUE y calcular características\n",
    "gdfs_pdi_denue_filtrados_test = {}\n",
    "for categoria, scian_codes_list in categorias_pdi_scian_dict.items():\n",
    "    if not scian_codes_list: \n",
    "        gdfs_pdi_denue_filtrados_test[categoria] = gpd.GeoDataFrame(columns=denue_gdf_test.columns, geometry=[], crs=denue_gdf_test.crs)\n",
    "        continue\n",
    "    scian_codes_str = [str(code).strip() for code in scian_codes_list]\n",
    "    gdfs_pdi_denue_filtrados_test[categoria] = denue_gdf_test[denue_gdf_test[COL_DENUE_SCIAN_CODE].isin(scian_codes_str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.A Contar PDIs DENUE en Radios\n",
    "for radio_m in RADIOS_M:\n",
    "    tiendas_buffer_gdf_test = tiendas_test_gdf.copy()\n",
    "    tiendas_buffer_gdf_test['geometry_buffer'] = tiendas_test_gdf.geometry.buffer(radio_m)\n",
    "    tiendas_buffer_gdf_test = tiendas_buffer_gdf_test.set_geometry('geometry_buffer')\n",
    "    for categoria, pdi_gdf_cat in tqdm(gdfs_pdi_denue_filtrados_test.items(), desc=f\"DENUE Conteo Test (R{radio_m}m)\"):\n",
    "        nombre_col = f'denue_conteo_{categoria}_{radio_m}m'\n",
    "        if not pdi_gdf_cat.empty:\n",
    "            try: joined = gpd.sjoin(tiendas_buffer_gdf_test[['TIENDA_ID', 'geometry_buffer']], pdi_gdf_cat[['geometry']], how='left', predicate='intersects')\n",
    "            except TypeError: joined = gpd.sjoin(tiendas_buffer_gdf_test[['TIENDA_ID', 'geometry_buffer']], pdi_gdf_cat[['geometry']], how='left', op='intersects')\n",
    "            conteo = joined.dropna(subset=['index_right']).groupby('TIENDA_ID').size()\n",
    "            tiendas_test_gdf[nombre_col] = tiendas_test_gdf['TIENDA_ID'].map(conteo).fillna(0).astype(int)\n",
    "        else: tiendas_test_gdf[nombre_col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9310c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.B Distancia al PDI DENUE más Cercano\n",
    "for categoria, pdi_gdf_cat in tqdm(gdfs_pdi_denue_filtrados_test.items(), desc=\"DENUE Dist Test\"):\n",
    "    nombre_col = f'denue_dist_{categoria}_cercano_m'\n",
    "    if not pdi_gdf_cat.empty and not pdi_gdf_cat.geometry.is_empty.all():\n",
    "        try:\n",
    "            sindex_pdi_cat = pdi_gdf_cat.sindex\n",
    "            distancias = []\n",
    "            for _, tienda_row in tiendas_test_gdf.iterrows():\n",
    "                tienda_geom = tienda_row.geometry\n",
    "                pos_indices = list(sindex_pdi_cat.intersection(tienda_geom.buffer(max(RADIOS_M) * 5).bounds)) # Buffer amplio\n",
    "                if not pos_indices: distancias.append(np.nan); continue\n",
    "                candidatos = pdi_gdf_cat.iloc[pos_indices]\n",
    "                if candidatos.empty: distancias.append(np.nan); continue\n",
    "                distancias.append(candidatos.geometry.distance(tienda_geom).min())\n",
    "            tiendas_test_gdf[nombre_col] = distancias\n",
    "            tiendas_test_gdf[nombre_col] = tiendas_test_gdf[nombre_col].fillna(99999).astype(float)\n",
    "        except Exception: # Fallback\n",
    "            tiendas_test_gdf[nombre_col] = tiendas_test_gdf.geometry.progress_apply(lambda g: pdi_gdf_cat.geometry.distance(g).min() if not pdi_gdf_cat.empty else np.nan).fillna(99999).astype(float)\n",
    "    else: tiendas_test_gdf[nombre_col] = 99999.0\n",
    "print(\"Características DENUE añadidas a tiendas_test_gdf.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43794fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PASO 3: Integración de Datos de OSM (Completo) ===\n",
    "print(\"\\n--- Iniciando Integración de Datos de OSM para TEST ---\")\n",
    "# 3.1 Cargar y preparar OSM POIs y Roads (similar al notebook 06)\n",
    "try:\n",
    "    osm_pois_gdf_raw_test = gpd.read_file(SHP_OSM_POIS_FILE)\n",
    "    osm_pois_gdf_test = osm_pois_gdf_raw_test.to_crs(CRS_PROYECTADO_OBJETIVO)\n",
    "    osm_roads_gdf_raw_test = gpd.read_file(SHP_OSM_ROADS_FILE)\n",
    "    osm_roads_gdf_test = osm_roads_gdf_raw_test.to_crs(CRS_PROYECTADO_OBJETIVO)\n",
    "    print(f\"OSM POIs y Roads para TEST preparados y reproyectados. Shapes: POIs {osm_pois_gdf_test.shape}, Roads {osm_roads_gdf_test.shape}\")\n",
    "except Exception as e: print(f\"Error cargando datos OSM para TEST: {e}\"); raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Filtrar POIs OSM y calcular características (conteo)\n",
    "# Usar `categorias_pois_osm` del notebook 06 (debes definirla aquí o importarla)\n",
    "categorias_pois_osm_dict = {\n",
    "    'paradas_autobus': ['bus_stop', 'bus_station'],\n",
    "    'estaciones_tren_metro': ['railway_station', 'subway_entrance', 'tram_stop', 'halt'],\n",
    "    'atm_cajeros': ['atm'],\n",
    "    'bancos_osm': ['bank'], # Puede solaparse con DENUE, pero OSM puede tener más ATMs\n",
    "    'parques_recreacion': ['park', 'playground', 'sports_centre', 'pitch', 'leisure_centre'],\n",
    "    'escuelas_osm': ['school', 'kindergarten'], # OSM puede tener guarderías ('kindergarten')\n",
    "    'universidades_colegios_osm': ['university', 'college']\n",
    "    # Añade más si son relevantes y OSM los mapea bien (ej. 'fuel' para gasolineras si DENUE no fue suficiente)\n",
    "}\n",
    "gdfs_pois_osm_filtrados_test = {}\n",
    "for categoria, fclass_values in categorias_pois_osm_dict.items():\n",
    "    gdfs_pois_osm_filtrados_test[categoria] = osm_pois_gdf_test[osm_pois_gdf_test[COL_OSM_POI_FCLASS].isin(fclass_values)]\n",
    "\n",
    "for radio_m in RADIOS_M: # Usar los mismos radios que para DENUE, o ajustar\n",
    "    tiendas_buffer_gdf_test = tiendas_test_gdf.copy()\n",
    "    tiendas_buffer_gdf_test['geometry_buffer'] = tiendas_test_gdf.geometry.buffer(radio_m)\n",
    "    tiendas_buffer_gdf_test = tiendas_buffer_gdf_test.set_geometry('geometry_buffer')\n",
    "    for categoria, pdi_gdf_cat in tqdm(gdfs_pois_osm_filtrados_test.items(), desc=f\"OSM POIs Conteo Test (R{radio_m}m)\"):\n",
    "        nombre_col = f'osm_conteo_{categoria}_{radio_m}m'\n",
    "        if not pdi_gdf_cat.empty:\n",
    "            try: joined = gpd.sjoin(tiendas_buffer_gdf_test[['TIENDA_ID', 'geometry_buffer']], pdi_gdf_cat[['geometry']], how='left', predicate='intersects')\n",
    "            except TypeError: joined = gpd.sjoin(tiendas_buffer_gdf_test[['TIENDA_ID', 'geometry_buffer']], pdi_gdf_cat[['geometry']], how='left', op='intersects')\n",
    "            conteo = joined.dropna(subset=['index_right']).groupby('TIENDA_ID').size()\n",
    "            tiendas_test_gdf[nombre_col] = tiendas_test_gdf['TIENDA_ID'].map(conteo).fillna(0).astype(int)\n",
    "        else: tiendas_test_gdf[nombre_col] = 0\n",
    "print(\"Características de conteo de POIs OSM añadidas a tiendas_test_gdf.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832883e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Calcular características de Red Vial OSM (distancia a vía principal)\n",
    "if osm_roads_gdf_test is not None and not osm_roads_gdf_test.empty:\n",
    "    tipos_via_principal_osm = ['motorway', 'trunk', 'primary', 'secondary'] # Ejemplo\n",
    "    vias_principales_osm_gdf_test = osm_roads_gdf_test[osm_roads_gdf_test[COL_OSM_ROAD_FCLASS].isin(tipos_via_principal_osm)]\n",
    "    if not vias_principales_osm_gdf_test.empty:\n",
    "        try:\n",
    "            sindex_vias_osm = vias_principales_osm_gdf_test.sindex\n",
    "            dist_via_osm = []\n",
    "            for _, tienda_row in tqdm(tiendas_test_gdf.iterrows(), total=len(tiendas_test_gdf), desc=\"OSM Dist Vía Test\"):\n",
    "                tienda_geom = tienda_row.geometry\n",
    "                pos_indices = list(sindex_vias_osm.intersection(tienda_geom.buffer(max(RADIOS_M) * 10).bounds)) # Buffer de búsqueda más grande\n",
    "                if not pos_indices: dist_via_osm.append(np.nan); continue\n",
    "                candidatas = vias_principales_osm_gdf_test.iloc[pos_indices]\n",
    "                if candidatas.empty: dist_via_osm.append(np.nan); continue\n",
    "                dist_via_osm.append(candidatas.geometry.distance(tienda_geom).min())\n",
    "            tiendas_test_gdf['osm_dist_via_principal_m'] = dist_via_osm\n",
    "            tiendas_test_gdf['osm_dist_via_principal_m'] = tiendas_test_gdf['osm_dist_via_principal_m'].fillna(99999).astype(float)\n",
    "        except Exception: # Fallback\n",
    "            tiendas_test_gdf['osm_dist_via_principal_m'] = tiendas_test_gdf.geometry.progress_apply(lambda g: vias_principales_osm_gdf_test.geometry.distance(g).min() if not vias_principales_osm_gdf_test.empty else np.nan).fillna(99999).astype(float)\n",
    "    else: tiendas_test_gdf['osm_dist_via_principal_m'] = 99999.0\n",
    "else: tiendas_test_gdf['osm_dist_via_principal_m'] = 99999.0\n",
    "print(\"Característica de distancia a vía principal OSM añadida a tiendas_test_gdf.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43474af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PASO 4: Integración de Datos del IMU (Completo) ===\n",
    "print(\"\\n--- Iniciando Integración de Datos del IMU para TEST ---\")\n",
    "try:\n",
    "    # Cargar IMU CSV (esto ya lo tienes bien con utf-8-sig)\n",
    "    imu_df_raw_test = pd.read_csv(ARCHIVO_IMU_CSV, encoding='utf-8-sig') \n",
    "    print(f\"IMU para TEST cargado. Shape: {imu_df_raw_test.shape}\")\n",
    "    \n",
    "    # Preparar la clave CVEGEO del IMU\n",
    "    imu_df_raw_test[COL_IMU_CVEGEO_KEY] = imu_df_raw_test[COL_IMU_CVEGEO_KEY].astype(str).str.strip()\n",
    "    \n",
    "    # Seleccionar solo las columnas que queremos del IMU para añadir\n",
    "    cols_imu_a_seleccionar = [COL_IMU_CVEGEO_KEY, COL_IMU_INDEX_VALUE, COL_IMU_GRADE_VALUE, COL_IMU_NAT_INDEX_VALUE]\n",
    "    # Verificar que todas estas columnas existan en imu_df_raw_test\n",
    "    missing_imu_cols = [col for col in cols_imu_a_seleccionar if col not in imu_df_raw_test.columns]\n",
    "    if missing_imu_cols:\n",
    "        raise ValueError(f\"Columnas IMU para seleccionar faltantes: {missing_imu_cols}. Disponibles: {imu_df_raw_test.columns.tolist()}\")\n",
    "    \n",
    "    imu_para_merge_df_test = imu_df_raw_test[cols_imu_a_seleccionar].drop_duplicates(subset=[COL_IMU_CVEGEO_KEY])\n",
    "    print(f\"IMU para merge preparado. Shape: {imu_para_merge_df_test.shape}\")\n",
    "\n",
    "    # ANTES del merge, verificamos y eliminamos columnas conflictivas en tiendas_test_gdf\n",
    "    print(\"\\nColumnas en tiendas_test_gdf ANTES del merge con IMU:\", tiendas_test_gdf.columns.tolist())\n",
    "    columnas_base_imu = ['IM_2020', 'GM_2020', 'IMN_2020'] # Nombres base sin sufijos\n",
    "    columnas_a_eliminar_de_tiendas = []\n",
    "\n",
    "    for col_base in columnas_base_imu:\n",
    "        if col_base in tiendas_test_gdf.columns:\n",
    "            print(f\"  Eliminando columna preexistente '{col_base}' de tiendas_test_gdf.\")\n",
    "            columnas_a_eliminar_de_tiendas.append(col_base)\n",
    "        if f\"{col_base}_x\" in tiendas_test_gdf.columns:\n",
    "            print(f\"  Eliminando columna preexistente '{col_base}_x' de tiendas_test_gdf.\")\n",
    "            columnas_a_eliminar_de_tiendas.append(f\"{col_base}_x\")\n",
    "        if f\"{col_base}_y\" in tiendas_test_gdf.columns:\n",
    "            print(f\"  Eliminando columna preexistente '{col_base}_y' de tiendas_test_gdf.\")\n",
    "            columnas_a_eliminar_de_tiendas.append(f\"{col_base}_y\")\n",
    "            \n",
    "    if columnas_a_eliminar_de_tiendas:\n",
    "        tiendas_test_gdf = tiendas_test_gdf.drop(columns=columnas_a_eliminar_de_tiendas)\n",
    "        print(\"  Columnas conflictivas eliminadas de tiendas_test_gdf.\")\n",
    "\n",
    "    # Ahora realizamos el merge. Las columnas del IMU deberían entrar con sus nombres originales.\n",
    "    tiendas_test_gdf = tiendas_test_gdf.merge(\n",
    "        imu_para_merge_df_test,\n",
    "        left_on='CVEGEO', \n",
    "        right_on=COL_IMU_CVEGEO_KEY, \n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"Merge con IMU completado. Shape de tiendas_test_gdf: {tiendas_test_gdf.shape}\")\n",
    "\n",
    "    tiendas_sin_imu_match = tiendas_test_gdf[COL_IMU_INDEX_VALUE].isnull().sum() \n",
    "    print(f\"Número de tiendas sin datos del IMU correspondientes: {tiendas_sin_imu_match} de {len(tiendas_test_gdf)}\")\n",
    "\n",
    "    # Si la columna clave del IMU (COL_IMU_CVEGEO_KEY) se añadió y es diferente de 'CVEGEO', y es redundante, eliminarla.\n",
    "    if COL_IMU_CVEGEO_KEY != 'CVEGEO' and COL_IMU_CVEGEO_KEY in tiendas_test_gdf.columns:\n",
    "        tiendas_test_gdf = tiendas_test_gdf.drop(columns=[COL_IMU_CVEGEO_KEY])\n",
    "        print(f\"Columna clave redundante del IMU '{COL_IMU_CVEGEO_KEY}' eliminada.\")\n",
    "    \n",
    "    print(\"\\nColumnas en tiendas_test_gdf DESPUÉS del merge con IMU:\")\n",
    "    print(tiendas_test_gdf.columns.tolist()) # Verificar que IM_2020, GM_2020, IMN_2020 estén sin sufijos\n",
    "\n",
    "except Exception as e: \n",
    "    print(f\"Error en la sección de integración del IMU: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa2cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PASO FINAL: Guardar el GeoDataFrame de TEST Enriquecido ---\n",
    "print(\"\\n--- Guardando GeoDataFrame de TEST Enriquecido ---\")\n",
    "print(\"Primeras filas del dataset de TEST final enriquecido:\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(tiendas_test_gdf.head())\n",
    "print(\"\\nColumnas finales del dataset de TEST:\")\n",
    "print(tiendas_test_gdf.columns.tolist())\n",
    "\n",
    "try:\n",
    "    tiendas_test_gdf.to_file(ARCHIVO_SALIDA_TEST_ENRIQUECIDO_GPKG, driver='GPKG', layer=LAYER_SALIDA_TEST_ENRIQUECIDO)\n",
    "    print(f\"\\nDataset de TEST enriquecido guardado en '{ARCHIVO_SALIDA_TEST_ENRIQUECIDO_GPKG}'\")\n",
    "except Exception as e: print(f\"Error al guardar archivo de salida TEST: {e}\"); raise\n",
    "\n",
    "print(\"\\n--- Enriquecimiento de DIM_TIENDA_TEST.csv Completado ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
