{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cce094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib # Para guardar preprocesadores\n",
    "import os\n",
    "import json # Para guardar nombres de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa817a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rutas y Nombres de Archivo (Configurables) ---\n",
    "RUTA_DATOS_ENRIQUECIDOS_TRAIN_INPUT = './datos/output/tiendas_train_completo_con_imu.gpkg'\n",
    "LAYER_TRAIN_INPUT = 'tiendas_completo_con_imu'\n",
    "\n",
    "RUTA_DATOS_ENRIQUECIDOS_TEST_OFICIAL_INPUT = './datos/output/tiendas_TEST_OFICIAL_enriquecido.gpkg'\n",
    "LAYER_TEST_OFICIAL_INPUT = 'tiendas_test_oficial_enriquecido' # Ajusta si el nombre de la capa es diferente\n",
    "\n",
    "RUTA_DATOS_PROCESADOS_OUTPUT = './datos/output/procesado_para_modelado/'\n",
    "if not os.path.exists(RUTA_DATOS_PROCESADOS_OUTPUT):\n",
    "    os.makedirs(RUTA_DATOS_PROCESADOS_OUTPUT)\n",
    "    print(f\"Directorio creado: {RUTA_DATOS_PROCESADOS_OUTPUT}\")\n",
    "# --- Fin de Configuración ---\n",
    "\n",
    "print(\"--- Iniciando Preprocesamiento Final para Modelado ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2afb1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Cargar Datasets Enriquecidos ---\n",
    "try:\n",
    "    df_train_gdf = gpd.read_file(RUTA_DATOS_ENRIQUECIDOS_TRAIN_INPUT, layer=LAYER_TRAIN_INPUT)\n",
    "    print(f\"Datos de entrenamiento enriquecidos cargados: {len(df_train_gdf)} tiendas.\")\n",
    "    \n",
    "    df_test_oficial_gdf = gpd.read_file(RUTA_DATOS_ENRIQUECIDOS_TEST_OFICIAL_INPUT, layer=LAYER_TEST_OFICIAL_INPUT)\n",
    "    print(f\"Datos de TEST OFICIAL enriquecidos cargados: {len(df_test_oficial_gdf)} tiendas.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar los archivos GeoPackage enriquecidos: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0463243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Selección Inicial de Características y Separación Target/Features ---\n",
    "cols_a_eliminar = [\n",
    "    'TIENDA_ID', \n",
    "    'geometry', \n",
    "    'VENTA_PROMEDIO_MENSUAL', # Solo en train_gdf\n",
    "    'Meta_venta',             # Solo en train_gdf\n",
    "    'N_MESES_CON_VENTA_EN_PERIODO', # Solo en train_gdf\n",
    "    'CVEGEO', \n",
    "    'NOM_ENT', 'NOM_MUN', 'NOM_LOC', 'NOM_AGEB', # Nombres geográficos\n",
    "    'CVE_AGEB ENT', # Si se coló del IMU y es redundante\n",
    "    \n",
    "    # NUEVAS COLUMNAS A AÑADIR PARA ELIMINAR (basado en tu warning)\n",
    "    'AGEB',         # Del warning\n",
    "    'DATASET',      # Del warning, definitivamente no es una feature\n",
    "    'ENTIDAD',      # Del warning\n",
    "    'LOC',          # Del warning\n",
    "    'MUN',          # Del warning\n",
    "    'PROMEDIO TOTAL', # Del warning\n",
    "    'TOTAL'           # Del warning\n",
    "    # ... y cualquier otra columna identificadora o de texto que no sea feature ...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar df_train_gdf como ejemplo de DataFrame\n",
    "df = df_train_gdf\n",
    "\n",
    "# Mostrar columnas con más del 50% de valores nulos\n",
    "null_threshold = 0.5\n",
    "null_cols = df.columns[df.isnull().mean() > null_threshold].tolist()\n",
    "print(\"Columnas con más del 50% de valores nulos:\", null_cols)\n",
    "\n",
    "# Columnas con un solo valor único\n",
    "constant_cols = [col for col in df.columns if df[col].nunique() <= 1]\n",
    "print(\"Columnas constantes (solo un valor):\", constant_cols)\n",
    "\n",
    "# Columnas dominadas por un solo valor\n",
    "low_variability_cols = [\n",
    "    col for col in df.columns\n",
    "    if df[col].value_counts(normalize=True, dropna=False).values[0] > 0.95\n",
    "]\n",
    "print(\"Columnas con más del 95% del mismo valor:\", low_variability_cols)\n",
    "\n",
    "# Columnas comúnmente irrelevantes para un modelo\n",
    "likely_useless = ['TIENDA_ID', 'LATITUD_NUM', 'LONGITUD_NUM', 'geometry']\n",
    "print(\"Columnas marcadas como no útiles:\", likely_useless)\n",
    "\n",
    "# Eliminar columnas del DataFrame\n",
    "cols_to_drop = cols_a_eliminar  # Usa la lista ya definida\n",
    "df_clean = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Verifica el nuevo tamaño\n",
    "print(\"Nuevas dimensiones del dataset limpio:\", df_clean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d0d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Procesamiento para el conjunto de ENTRENAMIENTO ---\n",
    "print(\"\\n--- Procesando datos de ENTRENAMIENTO ---\")\n",
    "cols_a_eliminar_train_existentes = [col for col in cols_a_eliminar if col in df_train_gdf.columns]\n",
    "df_train_pd = pd.DataFrame(df_train_gdf.drop(columns=cols_a_eliminar_train_existentes))\n",
    "\n",
    "if 'EXITO' not in df_train_pd.columns:\n",
    "    raise ValueError(\"La columna objetivo 'EXITO' no se encuentra en el DataFrame de entrenamiento.\")\n",
    "y_train_full = df_train_pd['EXITO']\n",
    "X_train_full_raw = df_train_pd.drop(columns=['EXITO'])\n",
    "print(f\"Features de entrenamiento (X_train_full_raw) shape: {X_train_full_raw.shape}\")\n",
    "\n",
    "# --- Procesamiento para el conjunto de TEST OFICIAL ---\n",
    "print(\"\\n--- Procesando datos de TEST OFICIAL ---\")\n",
    "cols_a_eliminar_test_existentes = [col for col in cols_a_eliminar if col in df_test_oficial_gdf.columns]\n",
    "# Asegurarse de no intentar eliminar 'EXITO' si no existe en el test set\n",
    "cols_a_eliminar_test_final = [c for c in cols_a_eliminar_test_existentes if c in df_test_oficial_gdf.columns and c != 'EXITO']\n",
    "\n",
    "X_test_oficial_raw = pd.DataFrame(df_test_oficial_gdf.drop(columns=cols_a_eliminar_test_final))\n",
    "print(f\"Features de TEST OFICIAL (X_test_oficial_raw) shape: {X_test_oficial_raw.shape}\")\n",
    "\n",
    "# Asegurar consistencia de columnas entre X_train_full_raw y X_test_oficial_raw\n",
    "# (X_test_oficial_raw no tendrá 'EXITO', lo cual es correcto)\n",
    "common_features = X_train_full_raw.columns.intersection(X_test_oficial_raw.columns)\n",
    "missing_in_test = X_train_full_raw.columns.difference(X_test_oficial_raw.columns)\n",
    "missing_in_train = X_test_oficial_raw.columns.difference(X_train_full_raw.columns)\n",
    "\n",
    "if len(missing_in_test) > 0:\n",
    "    print(f\"ADVERTENCIA: Columnas en TRAIN que NO están en TEST OFICIAL: {missing_in_test.tolist()}\")\n",
    "    # Esto podría ser un problema. Revisa el enriquecimiento del test.\n",
    "    # Por ahora, alinearemos X_test_oficial_raw a las columnas de X_train_full_raw, imputando faltantes si es necesario.\n",
    "    for col in missing_in_test: X_test_oficial_raw[col] = np.nan \n",
    "if len(missing_in_train) > 0:\n",
    "    print(f\"ADVERTENCIA: Columnas en TEST OFICIAL que NO están en TRAIN: {missing_in_train.tolist()}\")\n",
    "    X_test_oficial_raw = X_test_oficial_raw.drop(columns=missing_in_train) # Eliminar extras del test\n",
    "\n",
    "X_test_oficial_raw = X_test_oficial_raw[X_train_full_raw.columns] # Asegurar mismo orden y columnas\n",
    "print(f\"Shapes después de alinear columnas: X_train_full_raw: {X_train_full_raw.shape}, X_test_oficial_raw: {X_test_oficial_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfdbf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar consistencia de columnas entre X_train_full_raw y X_test_oficial_raw\n",
    "# (X_test_oficial_raw no tendrá 'EXITO', lo cual es correcto)\n",
    "common_features = X_train_full_raw.columns.intersection(X_test_oficial_raw.columns)\n",
    "missing_in_test = X_train_full_raw.columns.difference(X_test_oficial_raw.columns)\n",
    "missing_in_train = X_test_oficial_raw.columns.difference(X_train_full_raw.columns)\n",
    "\n",
    "if len(missing_in_test) > 0:\n",
    "    print(f\"ADVERTENCIA: Columnas en TRAIN que NO están en TEST OFICIAL: {missing_in_test.tolist()}\")\n",
    "    # Esto podría ser un problema. Revisa el enriquecimiento del test.\n",
    "    # Por ahora, alinearemos X_test_oficial_raw a las columnas de X_train_full_raw, imputando faltantes si es necesario.\n",
    "    for col in missing_in_test: X_test_oficial_raw[col] = np.nan \n",
    "if len(missing_in_train) > 0:\n",
    "    print(f\"ADVERTENCIA: Columnas en TEST OFICIAL que NO están en TRAIN: {missing_in_train.tolist()}\")\n",
    "    X_test_oficial_raw = X_test_oficial_raw.drop(columns=missing_in_train) # Eliminar extras del test\n",
    "\n",
    "X_test_oficial_raw = X_test_oficial_raw[X_train_full_raw.columns] # Asegurar mismo orden y columnas\n",
    "print(f\"Shapes después de alinear columnas: X_train_full_raw: {X_train_full_raw.shape}, X_test_oficial_raw: {X_test_oficial_raw.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Limpieza y Conversión de Tipos de Datos para Features ---\")\n",
    "\n",
    "# Lista de columnas que SABEMOS que deberían ser numéricas pero podrían tener strings\n",
    "# Esta lista debe ser exhaustiva, basada en tu conocimiento de los datos del Censo e IMU\n",
    "# ¡¡¡REVISA Y COMPLETA ESTA LISTA CON TODAS LAS COLUMNAS NUMÉRICAS DEL CENSO/IMU!!!\n",
    "columnas_a_convertir_a_numerico = [\n",
    "    'POBTOT', 'POBFEM', 'POBMAS', 'P_0A2', 'P_0A2_F', 'P_0A2_M', 'P_3YMAS', 'P_3YMAS_F', 'P_3YMAS_M',\n",
    "    'P_5YMAS', 'P_5YMAS_F', 'P_5YMAS_M', 'P_12YMAS', 'P_12YMAS_F', 'P_12YMAS_M', 'P_15YMAS', 'P_15YMAS_F',\n",
    "    'P_15YMAS_M', 'P_18YMAS', 'P_18YMAS_F', 'P_18YMAS_M', 'P_3A5', 'P_3A5_F', 'P_3A5_M', 'P_6A11',\n",
    "    'P_6A11_F', 'P_6A11_M', 'P_8A14', 'P_8A14_F', 'P_8A14_M', 'P_12A14', 'P_12A14_F', 'P_12A14_M',\n",
    "    'P_15A17', 'P_15A17_F', 'P_15A17_M', 'P_18A24', 'P_18A24_F', 'P_18A24_M', 'P_15A49_F', 'P_60YMAS',\n",
    "    'P_60YMAS_F', 'P_60YMAS_M', 'REL_H_M', 'POB0_14', 'POB15_64', 'POB65_MAS', 'PROM_HNV', 'PNACENT',\n",
    "    'PNACENT_F', 'PNACENT_M', 'PNACOE', 'PNACOE_F', 'PNACOE_M', 'PRES2015', 'PRES2015_F', 'PRES2015_M',\n",
    "    'PRESOE15', 'PRESOE15_F', 'PRESOE15_M', 'P3YM_HLI', 'P3YM_HLI_F', 'P3YM_HLI_M', 'P3HLINHE',\n",
    "    'P3HLINHE_F', 'P3HLINHE_M', 'P3HLI_HE', 'P3HLI_HE_F', 'P3HLI_HE_M', 'P5_HLI', 'P5_HLI_NHE',\n",
    "    'P5_HLI_HE', 'PHOG_IND', 'POB_AFRO', 'POB_AFRO_F', 'POB_AFRO_M', 'PCON_DISC', 'PCDISC_MOT',\n",
    "    'PCDISC_VIS', 'PCDISC_LENG', 'PCDISC_AUD', 'PCDISC_MOT2', 'PCDISC_MEN', 'PCON_LIMI', 'PCLIM_CSB',\n",
    "    'PCLIM_VIS', 'PCLIM_HACO', 'PCLIM_OAUD', 'PCLIM_MOT2', 'PCLIM_RE_CO', 'PCLIM_PMEN', 'PSIND_LIM',\n",
    "    'P3A5_NOA', 'P3A5_NOA_F', 'P3A5_NOA_M', 'P6A11_NOA', 'P6A11_NOAF', 'P6A11_NOAM', 'P12A14NOA',\n",
    "    'P12A14NOAF', 'P12A14NOAM', 'P15A17A', 'P15A17A_F', 'P15A17A_M', 'P18A24A', 'P18A24A_F',\n",
    "    'P18A24A_M', 'P8A14AN', 'P8A14AN_F', 'P8A14AN_M', 'P15YM_AN', 'P15YM_AN_F', 'P15YM_AN_M',\n",
    "    'P15YM_SE', 'P15YM_SE_F', 'P15YM_SE_M', 'P15PRI_IN', 'P15PRI_INF', 'P15PRI_INM', 'P15PRI_CO',\n",
    "    'P15PRI_COF', 'P15PRI_COM', 'P15SEC_IN', 'P15SEC_INF', 'P15SEC_INM', 'P15SEC_CO', 'P15SEC_COF',\n",
    "    'P15SEC_COM', 'P18YM_PB', 'P18YM_PB_F', 'P18YM_PB_M', 'GRAPROES', 'GRAPROES_F', 'GRAPROES_M',\n",
    "    'PEA', 'PEA_F', 'PEA_M', 'PE_INAC', 'PE_INAC_F', 'PE_INAC_M', 'POCUPADA', 'POCUPADA_F',\n",
    "    'POCUPADA_M', 'PDESOCUP', 'PDESOCUP_F', 'PDESOCUP_M', 'PSINDER', 'PDER_SS', 'PDER_IMSS',\n",
    "    'PDER_ISTE', 'PDER_ISTEE', 'PAFIL_PDOM', 'PDER_SEGP', 'PDER_IMSSB', 'PAFIL_IPRIV', 'PAFIL_OTRAI',\n",
    "    'P12YM_SOLT', 'P12YM_CASA', 'P12YM_SEPA', 'PCATOLICA', 'PRO_CRIEVA', 'POTRAS_REL', 'PSIN_RELIG',\n",
    "    'TOTHOG', 'HOGJEF_F', 'HOGJEF_M', 'POBHOG', 'PHOGJEF_F', 'PHOGJEF_M', 'VIVTOT', 'TVIVHAB',\n",
    "    'TVIVPAR', 'VIVPAR_HAB', 'VIVPARH_CV', 'TVIVPARHAB', 'VIVPAR_DES', 'VIVPAR_UT', 'OCUPVIVPAR',\n",
    "    'PROM_OCUP', 'PRO_OCUP_C', 'VPH_PISODT', 'VPH_PISOTI', 'VPH_1DOR', 'VPH_2YMASD', 'VPH_1CUART',\n",
    "    'VPH_2CUART', 'VPH_3YMASC', 'VPH_C_ELEC', 'VPH_S_ELEC', 'VPH_AGUADV', 'VPH_AEASP', 'VPH_AGUAFV',\n",
    "    'VPH_TINACO', 'VPH_CISTER', 'VPH_EXCSA', 'VPH_LETR', 'VPH_DRENAJ', 'VPH_NODREN', 'VPH_C_SERV',\n",
    "    'VPH_NDEAED', 'VPH_DSADMA', 'VPH_NDACMM', 'VPH_SNBIEN', 'VPH_REFRI', 'VPH_LAVAD', 'VPH_HMICRO',\n",
    "    'VPH_AUTOM', 'VPH_MOTO', 'VPH_BICI', 'VPH_RADIO', 'VPH_TV', 'VPH_PC', 'VPH_TELEF', 'VPH_CEL',\n",
    "    'VPH_INTER', 'VPH_STVP', 'VPH_SPMVPI', 'VPH_CVJ', 'VPH_SINRTV', 'VPH_SINLTC', 'VPH_SINCINT',\n",
    "    'VPH_SINTIC',\n",
    "    'IM_2020', 'IMN_2020' # GM_2020 es categórica\n",
    "    # No olvides las columnas de conteo y distancia de DENUE y OSM, esas ya deberían ser numéricas.\n",
    "    # Ej: 'denue_conteo_competidores_directos_conv_200m', 'denue_dist_competidores_directos_conv_cercano_m', etc.\n",
    "]\n",
    "\n",
    "# Aplicar a X_train_full_raw\n",
    "for col in columnas_a_convertir_a_numerico:\n",
    "    if col in X_train_full_raw.columns:\n",
    "        # Reemplazar no numéricos (como '*') con NaN para que to_numeric funcione\n",
    "        X_train_full_raw[col] = X_train_full_raw[col].replace('*', np.nan) \n",
    "        X_train_full_raw[col] = pd.to_numeric(X_train_full_raw[col], errors='coerce') # coerce pone NaN si no puede convertir\n",
    "    else:\n",
    "        print(f\"Advertencia: Columna '{col}' para convertir a numérico no encontrada en X_train_full_raw.\")\n",
    "\n",
    "# Aplicar a X_test_oficial_raw\n",
    "for col in columnas_a_convertir_a_numerico:\n",
    "    if col in X_test_oficial_raw.columns:\n",
    "        X_test_oficial_raw[col] = X_test_oficial_raw[col].replace('*', np.nan)\n",
    "        X_test_oficial_raw[col] = pd.to_numeric(X_test_oficial_raw[col], errors='coerce')\n",
    "    else:\n",
    "        print(f\"Advertencia: Columna '{col}' para convertir a numérico no encontrada en X_test_oficial_raw.\")\n",
    "\n",
    "print(\"Conversión a numérico completada (con 'coerce' para errores).\")\n",
    "print(\"Tipos de datos en X_train_full_raw después de conversión:\")\n",
    "print(X_train_full_raw[columnas_a_convertir_a_numerico].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Identificar Tipos de Columnas (Numéricas y Categóricas) ---\n",
    "# Primero identifica todas las que Pandas considera numéricas\n",
    "todas_columnas_numericas_inicial = X_train_full_raw.select_dtypes(include=np.number).columns.tolist()\n",
    "todas_columnas_categoricas_inicial = X_train_full_raw.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# --- AJUSTE AQUÍ PARA PLAZA_CVE ---\n",
    "# Definir las columnas que SON verdaderamente categóricas\n",
    "columnas_categoricas = ['NIVELSOCIOECONOMICO_DES', 'ENTORNO_DES', \n",
    "                        'SEGMENTO_MAESTRO_DESC', 'LID_UBICACION_TIENDA', \n",
    "                        'GM_2020', \n",
    "                        'PLAZA_CVE'] # Añadimos PLAZA_CVE aquí\n",
    "\n",
    "# Las numéricas serán todas las demás que Pandas leyó como número, EXCEPTO PLAZA_CVE\n",
    "columnas_numericas = [col for col in todas_columnas_numericas_inicial if col not in ['PLAZA_CVE']]\n",
    "# También asegúrate de que ninguna de las otras categóricas definidas arriba esté en la lista de numéricas por error.\n",
    "columnas_numericas = [col for col in columnas_numericas if col not in columnas_categoricas]\n",
    "\n",
    "\n",
    "# Antes de pasar PLAZA_CVE al OneHotEncoder, si es numérico (1, 2, 3...), \n",
    "# es buena idea convertirlo a string/object para que OHE lo trate correctamente como categorías distintas.\n",
    "# Esto se hace sobre X_train_full_raw y X_test_oficial_raw\n",
    "if 'PLAZA_CVE' in X_train_full_raw.columns:\n",
    "    X_train_full_raw['PLAZA_CVE'] = X_train_full_raw['PLAZA_CVE'].astype(str)\n",
    "if 'PLAZA_CVE' in X_test_oficial_raw.columns:\n",
    "    X_test_oficial_raw['PLAZA_CVE'] = X_test_oficial_raw['PLAZA_CVE'].astype(str)\n",
    "# --- FIN AJUSTE ---\n",
    "\n",
    "\n",
    "print(f\"\\nColumnas Numéricas DEFINITIVAS ({len(columnas_numericas)}): {columnas_numericas}\")\n",
    "print(f\"Columnas Categóricas DEFINITIVAS ({len(columnas_categoricas)}): {columnas_categoricas}\")\n",
    "\n",
    "# Imprimir cardinalidad de las categóricas DEFINITIVAS\n",
    "print(\"\\n--- Cardinalidad de Columnas Categóricas DEFINITIVAS (en X_train_full_raw) ---\")\n",
    "for col in columnas_categoricas:\n",
    "    if col in X_train_full_raw.columns:\n",
    "        num_unique = X_train_full_raw[col].nunique(dropna=False)\n",
    "        print(f\"Columna Categórica: '{col}', Número de Valores Únicos: {num_unique}\")\n",
    "    else:\n",
    "        print(f\"Advertencia: La columna categórica '{col}' no se encontró en X_train_full_raw.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922bc27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Crear Pipelines de Preprocesamiento ---\n",
    "pipeline_numerico = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')), # Imputar con mediana para robustez a outliers\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline_categorico = Pipeline([\n",
    "    # Paso 1: Imputar NaNs con una constante (string)\n",
    "    ('imputer_nan_to_str', SimpleImputer(strategy='constant', fill_value='Desconocido')), \n",
    "    # Paso 2: El OneHotEncoder se encargará de esta nueva categoría \"Desconocido\"\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocesador = ColumnTransformer([\n",
    "    ('num', pipeline_numerico, columnas_numericas),\n",
    "    ('cat', pipeline_categorico, columnas_categoricas)\n",
    "], remainder='drop') # 'drop' para eliminar columnas no especificadas (si las hubiera)\n",
    "                    # o 'passthrough' si quieres mantenerlas tal cual (menos común)\n",
    "print(\"\\nPipeline de preprocesamiento creado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2fc765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Dividir DATOS DE ENTRENAMIENTO en Entrenamiento y Validación Interna ---\n",
    "X_train_raw_split, X_val_raw_split, y_train, y_val = train_test_split(\n",
    "    X_train_full_raw, y_train_full, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_train_full\n",
    ")\n",
    "print(f\"\\nDatos de entrenamiento divididos: X_train_raw_split: {X_train_raw_split.shape}, X_val_raw_split: {X_val_raw_split.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03396ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Análisis de Cardinalidad de Columnas Categóricas (en X_train_raw_split) ---\")\n",
    "for col in columnas_categoricas:\n",
    "    if col in X_train_raw_split.columns:\n",
    "        num_unique = X_train_raw_split[col].nunique()\n",
    "        print(f\"Columna: '{col}', Número de Valores Únicos: {num_unique}\")\n",
    "        if num_unique > 50: # Un umbral arbitrario para señalar alta cardinalidad\n",
    "            print(f\"  ¡ALERTA! Alta cardinalidad en '{col}'. Valores de ejemplo: {X_train_raw_split[col].unique()[:5]}\")\n",
    "    else:\n",
    "        print(f\"Columna '{col}' no encontrada en X_train_raw_split para análisis de cardinalidad.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce21ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Ajustar el Preprocesador SOLO con X_train_raw_split ---\n",
    "print(\"\\nAjustando el preprocesador con X_train_raw_split...\")\n",
    "preprocesador.fit(X_train_raw_split)\n",
    "print(\"Preprocesador ajustado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbdc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Aplicar el Preprocesador a Todos los Conjuntos de Features ---\n",
    "print(\"\\nTransformando X_train_raw_split...\")\n",
    "X_train_procesado_arr = preprocesador.transform(X_train_raw_split)\n",
    "print(\"Transformando X_val_raw_split...\")\n",
    "X_val_procesado_arr = preprocesador.transform(X_val_raw_split)\n",
    "print(\"Transformando X_test_oficial_raw...\")\n",
    "X_test_oficial_procesado_arr = preprocesador.transform(X_test_oficial_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8f7291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Obtener Nombres de Features y Convertir a DataFrames (Opcional pero Recomendado) ---\n",
    "try:\n",
    "    nombres_features_onehot = preprocesador.named_transformers_['cat']['onehot']\\\n",
    "                               .get_feature_names_out(columnas_categoricas)\n",
    "    columnas_finales_procesadas = columnas_numericas + list(nombres_features_onehot)\n",
    "    \n",
    "    X_train_df = pd.DataFrame(X_train_procesado_arr, columns=columnas_finales_procesadas, index=X_train_raw_split.index)\n",
    "    X_val_df = pd.DataFrame(X_val_procesado_arr, columns=columnas_finales_procesadas, index=X_val_raw_split.index)\n",
    "    X_test_oficial_df = pd.DataFrame(X_test_oficial_procesado_arr, columns=columnas_finales_procesadas, index=X_test_oficial_raw.index)\n",
    "\n",
    "    print(f\"\\nShapes de DataFrames procesados: X_train_df: {X_train_df.shape}, X_val_df: {X_val_df.shape}, X_test_oficial_df: {X_test_oficial_df.shape}\")\n",
    "    print(\"Primeras filas de X_train_df procesado:\")\n",
    "    print(X_train_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error al obtener nombres de features o crear DataFrames procesados: {e}\")\n",
    "    print(\"Los datos X_train, X_val, X_test_oficial seguirán como arrays de NumPy.\")\n",
    "    # Si falla, los guardaremos como .npy y guardaremos los nombres de columna por separado si es posible\n",
    "    X_train_df, X_val_df, X_test_oficial_df = X_train_procesado_arr, X_val_procesado_arr, X_test_oficial_procesado_arr\n",
    "    if 'nombres_features_onehot' in locals(): # Intenta guardar nombres de features de todas formas\n",
    "         columnas_finales_procesadas = columnas_numericas + list(nombres_features_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd674fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. Guardar los Conjuntos de Datos Procesados y el Preprocesador ---\n",
    "print(\"\\n--- Guardando Datos Procesados y Preprocesador ---\")\n",
    "try:\n",
    "    if isinstance(X_train_df, pd.DataFrame):\n",
    "        X_train_df.to_csv(RUTA_DATOS_PROCESADOS_OUTPUT + 'X_train_procesado.csv', index=False)\n",
    "        X_val_df.to_csv(RUTA_DATOS_PROCESADOS_OUTPUT + 'X_val_procesado.csv', index=False)\n",
    "        X_test_oficial_df.to_csv(RUTA_DATOS_PROCESADOS_OUTPUT + 'X_test_oficial_procesado.csv', index=False)\n",
    "    else: # Guardar como .npy si son arrays\n",
    "        np.save(RUTA_DATOS_PROCESADOS_OUTPUT + 'X_train_procesado.npy', X_train_df)\n",
    "        np.save(RUTA_DATOS_PROCESADOS_OUTPUT + 'X_val_procesado.npy', X_val_df)\n",
    "        np.save(RUTA_DATOS_PROCESADOS_OUTPUT + 'X_test_oficial_procesado.npy', X_test_oficial_df)\n",
    "\n",
    "    y_train.to_csv(RUTA_DATOS_PROCESADOS_OUTPUT + 'y_train.csv', index=False, header=True)\n",
    "    y_val.to_csv(RUTA_DATOS_PROCESADOS_OUTPUT + 'y_val.csv', index=False, header=True)\n",
    "    \n",
    "    # Guardar los nombres de las columnas procesadas (muy útil para los notebooks de modelado)\n",
    "    if 'columnas_finales_procesadas' in locals():\n",
    "        with open(RUTA_DATOS_PROCESADOS_OUTPUT + 'columnas_procesadas_finales.json', 'w') as f:\n",
    "            json.dump(columnas_finales_procesadas, f)\n",
    "        print(\"Nombres de columnas procesadas guardados en 'columnas_procesadas_finales.json'\")\n",
    "    \n",
    "    joblib.dump(preprocesador, RUTA_DATOS_PROCESADOS_OUTPUT + 'preprocesador_completo.joblib')\n",
    "    print(f\"Preprocesador guardado en '{RUTA_DATOS_PROCESADOS_OUTPUT}preprocesador_completo.joblib'\")\n",
    "    print(\"\\nConjuntos de datos listos para modelado guardados.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al guardar los archivos procesados o el preprocesador: {e}\")\n",
    "\n",
    "print(\"\\n--- Preprocesamiento Final Completado ---\")\n",
    "print(f\"Los artefactos para modelado están en: {RUTA_DATOS_PROCESADOS_OUTPUT}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
